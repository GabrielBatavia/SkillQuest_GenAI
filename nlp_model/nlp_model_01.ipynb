{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Library dan Load Model",
   "id": "924ad08d5250d986"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from datasets import Dataset\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Mengatur kebijakan mixed precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "\n",
    "# Tentukan device untuk TensorFlow dan atur memory growth\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Load IndoBERT tokenizer dan model (TensorFlow version)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "bert_model = TFBertModel.from_pretrained(\"indobenchmark/indobert-base-p1\")\n"
   ],
   "id": "334405d13785e8a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fungsi Membaca dan Membersihkan Data",
   "id": "adead3061ad6f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def read_text_files(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read().strip().lower()\n",
    "                if content:  # Check if the file is not empty\n",
    "                    cleaned_content = clean_text(content)\n",
    "                    texts.append(cleaned_content)\n",
    "    return texts\n",
    "\n",
    "def clean_text(text):\n",
    "    unwanted_chars = ['*', '#', '_', ')', '(', '!', '?', '.', ',', '-']\n",
    "    for char in unwanted_chars:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n",
    "\n",
    "# Membaca teks dari folder dataset\n",
    "raw_texts = read_text_files('../Dataset/nlp_dataset')\n"
   ],
   "id": "48bb4205c63f8163",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenisasi dan Pembentukan Dataset",
   "id": "8777ca3189755510"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fungsi untuk tokenisasi\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Membaca teks dari folder dataset\n",
    "raw_texts = read_text_files('../Dataset/nlp_dataset')\n",
    "\n",
    "# Membagi teks menjadi batch kecil sebelum tokenisasi\n",
    "num_batches = 500  # Jumlah batch yang lebih banyak untuk mengurangi ukuran masing-masing batch\n",
    "batch_size = len(raw_texts) // num_batches\n",
    "\n",
    "# Membuat direktori untuk menyimpan subset dataset yang telah di-tokenize\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = (i + 1) * batch_size if (i + 1) * batch_size < len(raw_texts) else len(raw_texts)\n",
    "    batch_texts = raw_texts[start_idx:end_idx]\n",
    "\n",
    "    # Tokenisasi batch kecil\n",
    "    batch_dataset = Dataset.from_dict({\"text\": batch_texts})\n",
    "    tokenized_batch = batch_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    # Simpan tokenized batch ke disk\n",
    "    tokenized_batch.save_to_disk(f'../saved_model/nlp_saved/nlp_01/tokenized/tokenized_dataset_batch_{i}')\n",
    "\n"
   ],
   "id": "6b4c7699562c2703",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Membagi Dataset menjadi Beberapa Batch",
   "id": "bceac1c314ea55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "# Inisialisasi variabel untuk menyimpan semua dataset tokenized\n",
    "tokenized_datasets = []\n",
    "\n",
    "# Looping untuk memuat dataset dari folder yang telah di-tokenisasi\n",
    "for i in range(500):  # Sesuaikan dengan jumlah batch yang sudah Anda buat sebelumnya\n",
    "    path = f'../saved_model/nlp_saved/nlp_01/tokenized_dataset_batch_{i}'\n",
    "    if os.path.exists(path):\n",
    "        tokenized_batch = load_from_disk(path)\n",
    "        tokenized_datasets.append(tokenized_batch)\n",
    "    else:\n",
    "        print(f\"Path {path} tidak ditemukan.\")\n",
    "\n",
    "# Menggabungkan semua dataset menjadi satu\n",
    "if tokenized_datasets:\n",
    "    merged_dataset = concatenate_datasets(tokenized_datasets)\n",
    "\n",
    "    # Membagi dataset yang telah digabung menjadi batch yang lebih kecil\n",
    "    num_batches = 500  # Meningkatkan jumlah batch menjadi 500 untuk ukuran batch yang sangat kecil\n",
    "    batch_size = len(merged_dataset) // num_batches\n",
    "\n",
    "    # Membuat direktori untuk menyimpan subset dataset yang baru\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size if (i + 1) * batch_size < len(merged_dataset) else len(merged_dataset)\n",
    "        subset = merged_dataset.select(range(start_idx, end_idx))\n",
    "        subset.save_to_disk(f'../saved_model/nlp_saved/nlp_01/new_tokenized/new_tokenized_dataset_batch_{i}')\n",
    "else:\n",
    "    print(\"Tidak ada dataset yang berhasil dimuat.\")\n"
   ],
   "id": "1789ce7712b5139a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Definisi Model Custom untuk Sentence Embeddings",
   "id": "77f2a5d605851237"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BertAutoencoder(tf.keras.Model):\n",
    "    def __init__(self, bert_model):\n",
    "        super(BertAutoencoder, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dense = tf.keras.layers.Dense(bert_model.config.vocab_size, activation='softmax', dtype='float32')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.bert(**inputs)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        # Rekonstruksi input dari embeddings yang dihasilkan\n",
    "        reconstructed = self.dense(sequence_output)\n",
    "        return reconstructed\n",
    "\n",
    "# Instansiasi model autoencoder\n",
    "autoencoder_model = BertAutoencoder(bert_model)\n"
   ],
   "id": "79aa990139cb0aa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Memuat Dataset dan Fine-Tuning Model",
   "id": "f7476a9feab9b479"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Optimizer menggunakan LossScaleOptimizer dengan dynamic loss scale\n",
    "base_optimizer = Adam(learning_rate=2e-5)\n",
    "optimizer = LossScaleOptimizer(base_optimizer)\n",
    "\n",
    "# Compile model\n",
    "autoencoder_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Melatih model secara bertahap pada setiap batch kecil\n",
    "num_batches = 500  # Jumlah batch yang lebih banyak\n",
    "for i in range(num_batches):\n",
    "    print(f\"Training on batch {i+1}/{num_batches}\")\n",
    "    batch_dataset = load_from_disk(f'../saved_model/nlp_saved/nlp_01/new_tokenized/tokenized_dataset_batch_{i}')\n",
    "    \n",
    "    # Persiapan dataset untuk training\n",
    "    train_dataset = batch_dataset.to_tf_dataset(\n",
    "        columns=[\"input_ids\", \"attention_mask\"],\n",
    "        label_cols=[\"input_ids\"],\n",
    "        shuffle=True,\n",
    "        batch_size=2,  # Mengurangi batch size menjadi 2\n",
    "        collate_fn=None,\n",
    "    )\n",
    "\n",
    "    # Latih model\n",
    "    autoencoder_model.fit(train_dataset, epochs=3)\n",
    "\n",
    "# Simpan model setelah semua batch dilatih\n",
    "autoencoder_model.save_weights('../saved_model/nlp_saved/nlp_01/sentence_embedding_model_incremental')\n"
   ],
   "id": "5402e52708386c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ffa610b14316cca7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
