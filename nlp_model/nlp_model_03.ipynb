{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries and Set Mixed Precision Policy",
   "id": "64129554379b8cd0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-11T07:25:14.710127Z",
     "start_time": "2024-08-11T07:25:04.778139Z"
    }
   },
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import random \n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from datasets import Dataset, load_from_disk, concatenate_datasets\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "\n",
    "# Set mixed precision policy\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6\n",
      "WARNING:tensorflow:From C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\mixed_precision\\loss_scale.py:52: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set TensorFlow GPU Configuration",
   "id": "f00fb4cb1bf041e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T21:28:41.323264Z",
     "start_time": "2024-08-10T21:28:41.309723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure TensorFlow to use GPU and set memory growth\n",
    "#physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#if physical_devices:\n",
    "#    try:\n",
    "#        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#    except RuntimeError as e:\n",
    "#        print(e)\n"
   ],
   "id": "b0bd228a7eb1f0b0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T21:28:41.339360Z",
     "start_time": "2024-08-10T21:28:41.324572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure TensorFlow to use CPU (no specific configuration needed)\n",
    "import tensorflow as tf\n",
    "\n",
    "# List physical devices (just for verification)\n",
    "physical_devices = tf.config.list_physical_devices('CPU')\n",
    "print(f\"Available CPU devices: {physical_devices}\")\n"
   ],
   "id": "5da0ce22149d335d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load IndoBERT Tokenizer and Model",
   "id": "acb82e8df05a6107"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T21:28:43.119821Z",
     "start_time": "2024-08-10T21:28:41.341308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load IndoBERT tokenizer and model (TensorFlow version)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "bert_model = TFBertModel.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "bert_model.config.gradient_checkpointing = True\n"
   ],
   "id": "307ea52d07eac6fe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at indobenchmark/indobert-base-p1 were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at indobenchmark/indobert-base-p1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Generator for Reading Text Files",
   "id": "f6e528063cef3571"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T21:28:44.456476Z",
     "start_time": "2024-08-10T21:28:44.442241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generator to read text files and clean text\n",
    "def read_text_files_generator(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read().strip().lower()\n",
    "                if content:\n",
    "                    yield clean_text(content)\n",
    "\n",
    "def clean_text(text):\n",
    "    unwanted_chars = ['*', '#', '_', ')', '(', '!', '?', '.', ',', '-']\n",
    "    for char in unwanted_chars:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n"
   ],
   "id": "4bc02b371a2dd5f2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Process and Tokenize Text in Batches Using Generator",
   "id": "2dde80b94a12d2ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T21:29:10.701237Z",
     "start_time": "2024-08-10T21:28:45.958865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Function to tokenize and save batches\n",
    "def tokenize_and_save_batch(texts, batch_index, tokenizer, save_dir):\n",
    "    batch_dataset = Dataset.from_dict({\"text\": texts})\n",
    "    tokenized_batch = batch_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    tokenized_batch.save_to_disk(f'{save_dir}/tokenized_dataset_batch_{batch_index}')\n",
    "\n",
    "# Process texts in batches and save tokenized datasets\n",
    "def process_in_batches(folder_path, batch_size, tokenizer, save_dir):\n",
    "    batch_texts = []\n",
    "    batch_index = 0\n",
    "    for text in read_text_files_generator(folder_path):\n",
    "        batch_texts.append(text)\n",
    "        if len(batch_texts) >= batch_size:\n",
    "            tokenize_and_save_batch(batch_texts, batch_index, tokenizer, save_dir)\n",
    "            batch_texts = []\n",
    "            batch_index += 1\n",
    "    if batch_texts:\n",
    "        tokenize_and_save_batch(batch_texts, batch_index, tokenizer, save_dir)\n",
    "\n",
    "# Example usage:\n",
    "process_in_batches('../Dataset/nlp_dataset', batch_size=100, tokenizer=tokenizer, save_dir='../saved_model/nlp_saved/nlp_03/new_tokenized')\n"
   ],
   "id": "989fa0a8fe153880",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e43fb9c6d0604629bd4a1f690afd997c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "007602ec349148efb9945c56c9ca546b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f731b244cd74aa3b01496be283a8ed9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dabc0ceb3115405889c07a18e1079fc5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5631667ada5a4159b988c187288e3927"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a6582080ad34617be5acf31ed75896e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6c7b755b1244b38b5c66de6ece63847"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99e7b2bb78614260a0a2f19e4e1d3ae4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62daa545bdfe40649e9c3df31d96e1bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1274531fdcb74ad9b0324a32859afed0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3dd848102b604e778fb7fafe00a2f2fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a032174af6b4e5997b3ca0058add721"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61ce2c8a9116481cbc1f8bd8c742b832"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42b464841d4d40c682adb58e3c8a442e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a14da400c5b3457e92adc58ac48c437d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "654d19341dc64b2e88a38fd1d897f744"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "106bf7caacfa4a899958f6eba41d6ad3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "083375e75e23403b8a1c8dcfb742337d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a83ec9fb5f36438c99e7007ab8c7d32e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "451b57c0cf9941f1946e0b9974b6a65d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8783a024c8b3418c82ce51798c23c41a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85df5bcaaea3474baa86edd1d59ba841"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b458339c89c4d3ab16220fb85a1aae6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "669f9f600b1b4ca09faa21de6a0a944d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e34a154b963c44ebb8b6c6b329fcffbf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c1d0558be1d4cca984dfb87e0233491"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/92 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0f8dcfe73d74e5f9718b0373ca26414"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/92 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b65081aef98c445bb5a8f8f238054fba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Gradient Accumulation Setup",
   "id": "d69623e6990ddb71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T21:31:05.420421Z",
     "start_time": "2024-08-10T21:31:05.400398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AccumulatedOptimizer(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, optimizer, accumulation_steps=2):\n",
    "        super(AccumulatedOptimizer, self).__init__(name=\"AccumulatedOptimizer\")\n",
    "        self.optimizer = optimizer\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        self.iterations = 0\n",
    "\n",
    "    def apply_gradients(self, grads_and_vars, name=None, experimental_aggregate_gradients=True):\n",
    "        if not hasattr(self, \"grad_accumulation\"):\n",
    "            self.grad_accumulation = [tf.zeros_like(var) for _, var in grads_and_vars]\n",
    "\n",
    "        # Accumulate gradients\n",
    "        self.grad_accumulation = [\n",
    "            acc + grad / tf.cast(self.accumulation_steps, tf.float32)\n",
    "            for acc, (grad, _) in zip(self.grad_accumulation, grads_and_vars)\n",
    "        ]\n",
    "\n",
    "        self.iterations += 1\n",
    "\n",
    "        if self.iterations % self.accumulation_steps == 0:\n",
    "            # Apply accumulated gradients\n",
    "            self.optimizer.apply_gradients(\n",
    "                zip(self.grad_accumulation, [var for _, var in grads_and_vars])\n",
    "            )\n",
    "            # Reset accumulation\n",
    "            self.grad_accumulation = [\n",
    "                tf.zeros_like(var) for _, var in grads_and_vars\n",
    "            ]\n"
   ],
   "id": "e9d293ea7626d5d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Caching and Mapping Dataset",
   "id": "8b35a25c792f79e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T21:31:06.507783Z",
     "start_time": "2024-08-10T21:31:06.487930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataset with caching for efficient memory usage\n",
    "def load_and_prepare_dataset_with_caching(batch_index, save_dir):\n",
    "    batch_dataset = load_from_disk(f'{save_dir}/tokenized_dataset_batch_{batch_index}')\n",
    "    \n",
    "    # Caching to improve efficiency\n",
    "    dataset = batch_dataset.to_tf_dataset(\n",
    "        columns=[\"input_ids\", \"attention_mask\"],\n",
    "        label_cols=[\"input_ids\"],\n",
    "        shuffle=True,\n",
    "        batch_size=1,  # Use batch size of 1 to minimize memory use\n",
    "        collate_fn=None,\n",
    "    ).cache().map(lambda x, y: (x, y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ],
   "id": "9385a3542c231769",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define and Compile BertAutoencoder Model",
   "id": "abd30c5f49264657"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T21:31:08.073088Z",
     "start_time": "2024-08-10T21:31:08.044385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the BertAutoencoder model\n",
    "class BertAutoencoder(tf.keras.Model):\n",
    "    def __init__(self, bert_model):\n",
    "        super(BertAutoencoder, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dense = tf.keras.layers.Dense(bert_model.config.vocab_size, activation='softmax', dtype='float32')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.bert(**inputs)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        reconstructed = self.dense(sequence_output)\n",
    "        return reconstructed\n",
    "\n",
    "# Instantiate the model\n",
    "autoencoder_model = BertAutoencoder(bert_model)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(learning_rate=2e-5)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n"
   ],
   "id": "f5cc36747e8bf2db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
      "  opt = tf.keras.mixed_precision.LossScaleOptimizer(opt)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implement Gradient Accumulation in Training Loop",
   "id": "222bfb27efa2ee2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T21:41:40.010140Z",
     "start_time": "2024-08-10T21:37:31.922240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the training step wrapped in tf.function\n",
    "@tf.function\n",
    "def train_step(model, optimizer, x_batch_train, y_batch_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x_batch_train, training=True)\n",
    "        loss_value = model.compiled_loss(y_batch_train, logits)\n",
    "    \n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    return loss_value\n",
    "\n",
    "\n",
    "# Tambahkan variabel untuk menyimpan loss\n",
    "loss_history = []\n",
    "\n",
    "def train_model_with_optimization_and_save(model, num_batches, save_dir, save_model_path, accumulation_steps=16):\n",
    "    for i in range(num_batches):\n",
    "        print(f\"Training on batch {i+1}/{num_batches}\")\n",
    "        train_dataset = load_and_prepare_dataset_with_caching(i, save_dir)\n",
    "        \n",
    "        step = 0\n",
    "        for x_batch_train, y_batch_train in train_dataset:\n",
    "            step += 1\n",
    "            # Perform training step with tf.function\n",
    "            loss_value = train_step(model, optimizer, x_batch_train, y_batch_train)\n",
    "            \n",
    "            if step % accumulation_steps == 0:\n",
    "                # Log every accumulation step\n",
    "                print(f\"Step {step}: Loss: {loss_value.numpy()}\")\n",
    "    \n",
    "    # Simpan model setelah pelatihan selesai\n",
    "    model.save(save_model_path)\n",
    "    print(f\"Model saved to {save_model_path}\")\n",
    "\n",
    "# Contoh penggunaan\n",
    "train_model_with_optimization_and_save(autoencoder_model, 13, '../saved_model/nlp_saved/nlp_03/new_tokenized', '../saved_model/nlp_saved/nlp_03/saved_model_directory', accumulation_steps=8)"
   ],
   "id": "588c17a98b1ace82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 1/13\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step 8: Loss: 2.8689424991607666\n",
      "Step 16: Loss: 1.1023693084716797\n",
      "Step 24: Loss: 2.145442008972168\n",
      "Step 32: Loss: 0.835431694984436\n",
      "Step 40: Loss: 1.0290364027023315\n",
      "Step 48: Loss: 1.8634346723556519\n",
      "Step 56: Loss: 2.9927496910095215\n",
      "Step 64: Loss: 2.7182886600494385\n",
      "Step 72: Loss: 1.1427456140518188\n",
      "Step 80: Loss: 2.0275275707244873\n",
      "Step 88: Loss: 1.785470962524414\n",
      "Step 96: Loss: 0.552955150604248\n",
      "Training on batch 2/13\n",
      "Step 8: Loss: 1.2938839197158813\n",
      "Step 16: Loss: 1.6366604566574097\n",
      "Step 24: Loss: 1.5060174465179443\n",
      "Step 32: Loss: 0.6831838488578796\n",
      "Step 40: Loss: 1.0494234561920166\n",
      "Step 48: Loss: 1.0559026002883911\n",
      "Step 56: Loss: 1.1589046716690063\n",
      "Step 64: Loss: 0.5934618711471558\n",
      "Step 72: Loss: 1.1583428382873535\n",
      "Step 80: Loss: 1.5180381536483765\n",
      "Step 88: Loss: 1.230949878692627\n",
      "Step 96: Loss: 0.6484699249267578\n",
      "Training on batch 3/13\n",
      "Step 8: Loss: 2.2283945083618164\n",
      "Step 16: Loss: 1.215683937072754\n",
      "Step 24: Loss: 2.823082685470581\n",
      "Step 32: Loss: 0.9666798114776611\n",
      "Step 40: Loss: 0.9627065658569336\n",
      "Step 48: Loss: 1.716491937637329\n",
      "Step 56: Loss: 1.2521635293960571\n",
      "Step 64: Loss: 0.672232985496521\n",
      "Step 72: Loss: 1.02260422706604\n",
      "Step 80: Loss: 0.6933138370513916\n",
      "Step 88: Loss: 1.9120136499404907\n",
      "Step 96: Loss: 1.44426691532135\n",
      "Training on batch 4/13\n",
      "Step 8: Loss: 1.5111303329467773\n",
      "Step 16: Loss: 0.9437941312789917\n",
      "Step 24: Loss: 1.4621670246124268\n",
      "Step 32: Loss: 0.9222285151481628\n",
      "Step 40: Loss: 1.146802306175232\n",
      "Step 48: Loss: 0.4741224944591522\n",
      "Step 56: Loss: 1.506791114807129\n",
      "Step 64: Loss: 0.6614632606506348\n",
      "Step 72: Loss: 1.0072979927062988\n",
      "Step 80: Loss: 1.4953796863555908\n",
      "Step 88: Loss: 0.9351720213890076\n",
      "Step 96: Loss: 0.5662811994552612\n",
      "Training on batch 5/13\n",
      "Step 8: Loss: 0.6310771703720093\n",
      "Step 16: Loss: 0.6690242886543274\n",
      "Step 24: Loss: 0.6053627729415894\n",
      "Step 32: Loss: 0.6190725564956665\n",
      "Step 40: Loss: 1.0658798217773438\n",
      "Step 48: Loss: 1.1678332090377808\n",
      "Step 56: Loss: 0.6096514463424683\n",
      "Step 64: Loss: 0.61384516954422\n",
      "Step 72: Loss: 0.8125902414321899\n",
      "Step 80: Loss: 0.8329424262046814\n",
      "Step 88: Loss: 1.6308797597885132\n",
      "Step 96: Loss: 0.5589275360107422\n",
      "Training on batch 6/13\n",
      "Step 8: Loss: 1.0280431509017944\n",
      "Step 16: Loss: 0.44741290807724\n",
      "Step 24: Loss: 1.0661373138427734\n",
      "Step 32: Loss: 0.9567462801933289\n",
      "Step 40: Loss: 0.5619131326675415\n",
      "Step 48: Loss: 0.999967634677887\n",
      "Step 56: Loss: 0.6879668235778809\n",
      "Step 64: Loss: 1.4364339113235474\n",
      "Step 72: Loss: 0.8869873285293579\n",
      "Step 80: Loss: 0.4093778133392334\n",
      "Step 88: Loss: 0.5371782779693604\n",
      "Step 96: Loss: 0.616288423538208\n",
      "Training on batch 7/13\n",
      "Step 8: Loss: 0.4918575584888458\n",
      "Step 16: Loss: 0.3887636065483093\n",
      "Step 24: Loss: 0.5743915438652039\n",
      "Step 32: Loss: 0.37965327501296997\n",
      "Step 40: Loss: 0.4521847367286682\n",
      "Step 48: Loss: 0.3135684132575989\n",
      "Step 56: Loss: 0.48360875248908997\n",
      "Step 64: Loss: 0.832007110118866\n",
      "Step 72: Loss: 0.6078636050224304\n",
      "Step 80: Loss: 0.9005199670791626\n",
      "Step 88: Loss: 0.22535870969295502\n",
      "Step 96: Loss: 0.4003581404685974\n",
      "Training on batch 8/13\n",
      "Step 8: Loss: 0.5745740532875061\n",
      "Step 16: Loss: 0.5121095180511475\n",
      "Step 24: Loss: 0.38541680574417114\n",
      "Step 32: Loss: 0.4013623595237732\n",
      "Step 40: Loss: 0.3070492148399353\n",
      "Step 48: Loss: 0.5192313194274902\n",
      "Step 56: Loss: 0.5035080909729004\n",
      "Step 64: Loss: 0.38886919617652893\n",
      "Step 72: Loss: 0.43371444940567017\n",
      "Step 80: Loss: 0.32872676849365234\n",
      "Step 88: Loss: 0.31277135014533997\n",
      "Step 96: Loss: 0.3304003179073334\n",
      "Training on batch 9/13\n",
      "Step 8: Loss: 1.2779138088226318\n",
      "Step 16: Loss: 1.02516770362854\n",
      "Step 24: Loss: 0.4161760210990906\n",
      "Step 32: Loss: 0.281426340341568\n",
      "Step 40: Loss: 0.3623499274253845\n",
      "Step 48: Loss: 1.3500957489013672\n",
      "Step 56: Loss: 1.3417819738388062\n",
      "Step 64: Loss: 0.5284044742584229\n",
      "Step 72: Loss: 0.3160419762134552\n",
      "Step 80: Loss: 1.1064283847808838\n",
      "Step 88: Loss: 0.3550885021686554\n",
      "Step 96: Loss: 0.46061134338378906\n",
      "Training on batch 10/13\n",
      "Step 8: Loss: 0.545075535774231\n",
      "Step 16: Loss: 0.5471258759498596\n",
      "Step 24: Loss: 0.4562262296676636\n",
      "Step 32: Loss: 1.0290166139602661\n",
      "Step 40: Loss: 0.5037451982498169\n",
      "Step 48: Loss: 0.5433428287506104\n",
      "Step 56: Loss: 0.3809833526611328\n",
      "Step 64: Loss: 1.612335205078125\n",
      "Step 72: Loss: 0.3070613145828247\n",
      "Step 80: Loss: 0.6536434292793274\n",
      "Step 88: Loss: 0.43115338683128357\n",
      "Step 96: Loss: 0.42932504415512085\n",
      "Training on batch 11/13\n",
      "Step 8: Loss: 0.6328415274620056\n",
      "Step 16: Loss: 0.4375535249710083\n",
      "Step 24: Loss: 0.952126145362854\n",
      "Step 32: Loss: 0.9218447804450989\n",
      "Step 40: Loss: 1.233797550201416\n",
      "Step 48: Loss: 1.109333872795105\n",
      "Step 56: Loss: 0.4277504086494446\n",
      "Step 64: Loss: 0.35811692476272583\n",
      "Step 72: Loss: 0.39488816261291504\n",
      "Step 80: Loss: 0.3575948476791382\n",
      "Step 88: Loss: 0.8263849020004272\n",
      "Step 96: Loss: 0.8687297105789185\n",
      "Training on batch 12/13\n",
      "Step 8: Loss: 0.2779129147529602\n",
      "Step 16: Loss: 1.3747410774230957\n",
      "Step 24: Loss: 1.034544825553894\n",
      "Step 32: Loss: 0.2619924247264862\n",
      "Step 40: Loss: 0.48857566714286804\n",
      "Step 48: Loss: 1.3192033767700195\n",
      "Step 56: Loss: 1.2604334354400635\n",
      "Step 64: Loss: 1.1204310655593872\n",
      "Step 72: Loss: 0.46928322315216064\n",
      "Step 80: Loss: 0.16460618376731873\n",
      "Step 88: Loss: 0.9488121867179871\n",
      "Step 96: Loss: 0.5708936452865601\n",
      "Training on batch 13/13\n",
      "Step 8: Loss: 0.3559390902519226\n",
      "Step 16: Loss: 0.23047010600566864\n",
      "Step 24: Loss: 0.312566876411438\n",
      "Step 32: Loss: 0.17142993211746216\n",
      "Step 40: Loss: 0.3537304699420929\n",
      "Step 48: Loss: 0.36493992805480957\n",
      "Step 56: Loss: 0.1583324670791626\n",
      "Step 64: Loss: 0.22825142741203308\n",
      "Step 72: Loss: 0.2697197198867798\n",
      "Step 80: Loss: 0.142208531498909\n",
      "Step 88: Loss: 0.12837691605091095\n",
      "Step 96: Loss: 0.24411988258361816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saved_model/nlp_saved/nlp_03/saved_model_directory\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saved_model/nlp_saved/nlp_03/saved_model_directory\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../saved_model/nlp_saved/nlp_03/saved_model_directory\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T21:42:46.505528Z",
     "start_time": "2024-08-10T21:42:45.723151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ],
   "id": "687a8eba212ac379",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:25:27.961482Z",
     "start_time": "2024-08-11T07:25:14.712133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model yang telah disimpan\n",
    "loaded_model = tf.keras.models.load_model('../saved_model/nlp_saved/nlp_03/saved_model_directory')\n",
    "\n",
    "# Pastikan model sudah ter-load dengan benar\n",
    "loaded_model.summary()"
   ],
   "id": "faf105e0f6cb8cd4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\configuration_utils.py:365: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The two structures don't have the same nested structure.\n\nFirst structure: type=tuple str=(({'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name=None), 'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids/input_ids')}, None, None, None, None, None, None, None, None, None, None, None, None, False), {})\n\nSecond structure: type=tuple str=((TensorSpec(shape=(None, 512), dtype=tf.int64, name='input_ids'), TensorSpec(shape=(None, 512), dtype=tf.int64, name='attention_mask'), None, None, None, None, None, None, None, None, None, None, None, False), {})\n\nMore specifically: Substructure \"type=dict str={'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name=None), 'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids/input_ids')}\" is a sequence, while substructure \"type=TensorSpec str=TensorSpec(shape=(None, 512), dtype=tf.int64, name='input_ids')\" is not\nEntire first structure:\n(({'attention_mask': ., 'token_type_ids': ., 'input_ids': .}, ., ., ., ., ., ., ., ., ., ., ., ., .), {})\nEntire second structure:\n((., ., ., ., ., ., ., ., ., ., ., ., ., .), {})",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Load model yang telah disimpan\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m loaded_model \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../saved_model/nlp_saved/nlp_03/saved_model_directory\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Pastikan model sudah ter-load dengan benar\u001B[39;00m\n\u001B[0;32m      5\u001B[0m loaded_model\u001B[38;5;241m.\u001B[39msummary()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\nest.py:570\u001B[0m, in \u001B[0;36massert_same_structure\u001B[1;34m(nest1, nest2, check_types, expand_composites)\u001B[0m\n\u001B[0;32m    568\u001B[0m str1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(map_structure(\u001B[38;5;28;01mlambda\u001B[39;00m _: _DOT, nest1))\n\u001B[0;32m    569\u001B[0m str2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(map_structure(\u001B[38;5;28;01mlambda\u001B[39;00m _: _DOT, nest2))\n\u001B[1;32m--> 570\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(e)(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    571\u001B[0m               \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEntire first structure:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    572\u001B[0m               \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEntire second structure:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    573\u001B[0m               \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mstr\u001B[39m(e), str1, str2))\n",
      "\u001B[1;31mValueError\u001B[0m: The two structures don't have the same nested structure.\n\nFirst structure: type=tuple str=(({'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name=None), 'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids/input_ids')}, None, None, None, None, None, None, None, None, None, None, None, None, False), {})\n\nSecond structure: type=tuple str=((TensorSpec(shape=(None, 512), dtype=tf.int64, name='input_ids'), TensorSpec(shape=(None, 512), dtype=tf.int64, name='attention_mask'), None, None, None, None, None, None, None, None, None, None, None, False), {})\n\nMore specifically: Substructure \"type=dict str={'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name=None), 'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids/input_ids')}\" is a sequence, while substructure \"type=TensorSpec str=TensorSpec(shape=(None, 512), dtype=tf.int64, name='input_ids')\" is not\nEntire first structure:\n(({'attention_mask': ., 'token_type_ids': ., 'input_ids': .}, ., ., ., ., ., ., ., ., ., ., ., ., .), {})\nEntire second structure:\n((., ., ., ., ., ., ., ., ., ., ., ., ., .), {})"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eea95c33b51a01f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
