{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-14T20:42:36.508729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import neptune\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "import json\n",
    "\n",
    "# Menggunakan GPU jika tersedia, fallback ke CPU jika OutOfMemoryError terjadi\n",
    "def load_model(model_name):\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')\n",
    "        print(\"Model loaded on GPU\")\n",
    "    except RuntimeError as e:\n",
    "        if 'out of memory' in str(e):\n",
    "            print(\"Out of memory error on GPU. Loading model on CPU...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name).to('cpu')\n",
    "        else:\n",
    "            raise e\n",
    "    return model\n",
    "\n",
    "# Load dataset dari file JSON\n",
    "def load_dataset(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def fine_tune_model(model_name, dataset_file, output_dir):\n",
    "    # Load model dan tokenizer\n",
    "    model = load_model(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Load dataset\n",
    "    data = load_dataset(dataset_file)\n",
    "\n",
    "    # Preprocess dataset\n",
    "    inputs = tokenizer([conv['conversations'][0]['value'] for conv in data], return_tensors='pt', truncation=True, padding='max_length', max_length=64)\n",
    "    labels = tokenizer([conv['conversations'][1]['value'] for conv in data], return_tensors='pt', truncation=True, padding='max_length', max_length=64)\n",
    "\n",
    "    class CustomDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "            item['labels'] = self.labels['input_ids'][idx]\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels['input_ids'])\n",
    "\n",
    "    dataset = CustomDataset(inputs, labels)\n",
    "\n",
    "    # Logging hyperparameter ke Neptune\n",
    "    params = {\n",
    "        \"model_name\": model_name,\n",
    "        \"batch_size\": 1,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"gradient_accumulation_steps\": 16,\n",
    "    }\n",
    "    run[\"parameters\"] = params\n",
    "\n",
    "    # Mendefinisikan argumen training\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=params[\"batch_size\"],\n",
    "        gradient_accumulation_steps=params[\"gradient_accumulation_steps\"],\n",
    "        num_train_epochs=params[\"num_train_epochs\"],\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=250,\n",
    "        logging_dir='./logs',\n",
    "        report_to=\"none\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    run.stop()\n",
    "\n",
    "    print(f\"Model fine-tuning selesai dan disimpan di: {output_dir}\")\n",
    "\n",
    "# Inisialisasi Neptune\n",
    "run = neptune.init_run(\n",
    "    project=\"gabrielbatavia/Pintarpath\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhYmNjYmMxMi01NTZiLTRmMzgtOTg3ZC1hMDk0YjJhZGI3MzQifQ==\",\n",
    "    name=\"Fine-tuning Nusantara Chat Model\",\n",
    "    tags=[\"fine-tuning\", \"NLP\", \"Transformers\"],\n",
    "    dependencies=\"infer\",\n",
    "    capture_hardware_metrics=True,\n",
    ")\n",
    "\n",
    "# Contoh penggunaan\n",
    "model_name = \"kalisai/Nusantara-0.8b-Indo-Chat\"\n",
    "dataset_file = '../Dataset/nusantara_dataset/output2.json'\n",
    "output_dir = '../saved_model/fine-tuned-model'\n",
    "fine_tune_model(model_name, dataset_file, output_dir)\n"
   ],
   "id": "f2844e1a669fb39d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/gabrielbatavia/Pintarpath/e/PIN-3\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5568c8c60796b11f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## code di gcp",
   "id": "6b16c791c428baed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import neptune\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "import subprocess\n",
    "\n",
    "# Fungsi untuk mendownload dataset dari bucket GCS\n",
    "def download_from_gcs(bucket_path, local_path):\n",
    "    subprocess.run(['gsutil', 'cp', bucket_path, local_path])\n",
    "\n",
    "# Fungsi untuk mengupload model ke bucket GCS\n",
    "def upload_to_gcs(local_path, bucket_path):\n",
    "    subprocess.run(['gsutil', 'cp', '-r', local_path, bucket_path])\n",
    "\n",
    "# Menggunakan GPU jika tersedia, fallback ke CPU jika OutOfMemoryError terjadi\n",
    "def load_model(model_name):\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(\"Model loaded on GPU\" if torch.cuda.is_available() else \"Model loaded on CPU\")\n",
    "    except RuntimeError as e:\n",
    "        if 'out of memory' in str(e):\n",
    "            print(\"Out of memory error on GPU. Loading model on CPU...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name).to('cpu')\n",
    "        else:\n",
    "            raise e\n",
    "    return model\n",
    "\n",
    "# Load dataset dari file JSON\n",
    "def load_dataset(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Fungsi evaluasi accuracy\n",
    "def compute_accuracy(pred):\n",
    "    predictions = pred.predictions.argmax(-1)\n",
    "    labels = pred.label_ids\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "def fine_tune_model(model_name, dataset_file, output_dir):\n",
    "    # Load model dan tokenizer\n",
    "    model = load_model(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Load dataset\n",
    "    data = load_dataset(dataset_file)\n",
    "\n",
    "    # Preprocess dataset\n",
    "    inputs = tokenizer([conv['conversations'][0]['value'] for conv in data], return_tensors='pt', truncation=True, padding='max_length', max_length=64)\n",
    "    labels = tokenizer([conv['conversations'][1]['value'] for conv in data], return_tensors='pt', truncation=True, padding='max_length', max_length=64)\n",
    "\n",
    "    class CustomDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "            item['labels'] = self.labels['input_ids'][idx]\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels['input_ids'])\n",
    "\n",
    "    dataset = CustomDataset(inputs, labels)\n",
    "\n",
    "    # Logging hyperparameter ke Neptune\n",
    "    params = {\n",
    "        \"model_name\": model_name,\n",
    "        \"batch_size\": 1,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"gradient_accumulation_steps\": 16,\n",
    "    }\n",
    "    run[\"parameters\"] = params\n",
    "\n",
    "    # Mendefinisikan argumen training\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=params[\"batch_size\"],\n",
    "        gradient_accumulation_steps=params[\"gradient_accumulation_steps\"],\n",
    "        num_train_epochs=params[\"num_train_epochs\"],\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=250,\n",
    "        logging_dir='./logs',\n",
    "        report_to=\"none\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "        compute_metrics=compute_accuracy,  # Tambahkan fungsi compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    run.stop()\n",
    "\n",
    "    print(f\"Model fine-tuning selesai dan disimpan di: {output_dir}\")\n",
    "\n",
    "# Inisialisasi Neptune\n",
    "run = neptune.init_run(\n",
    "    project=\"gabrielbatavia/Pintarpath\",\n",
    "    api_token=os.getenv(\"NEPTUNE_API_TOKEN\"),  # Gunakan environment variable untuk keamanan\n",
    "    name=\"Fine-tuning Nusantara Chat Model\",\n",
    "    tags=[\"fine-tuning\", \"NLP\", \"Transformers\"],\n",
    "    dependencies=\"infer\",\n",
    "    capture_hardware_metrics=True,\n",
    ")\n",
    "\n",
    "# Contoh penggunaan\n",
    "bucket_dataset_path = 'gs://pintarpath-trying/output2.json'  # Path di GCS\n",
    "local_dataset_file = './output2.json'  # Path lokal untuk dataset\n",
    "bucket_output_dir = 'gs://pintarpath-trying/saved_model/fine-tuned-model'  # Path output di GCS\n",
    "local_output_dir = './fine-tuned-model'  # Path lokal untuk menyimpan model sebelum upload\n",
    "\n",
    "# Download dataset dari GCS\n",
    "download_from_gcs(bucket_dataset_path, local_dataset_file)\n",
    "\n",
    "# Fine-tuning model\n",
    "fine_tune_model(model_name=\"kalisai/Nusantara-0.8b-Indo-Chat\", dataset_file=local_dataset_file, output_dir=local_output_dir)\n",
    "\n",
    "# Upload model ke GCS\n",
    "upload_to_gcs(local_output_dir, bucket_output_dir)\n"
   ],
   "id": "c60c666aeded9882"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2f3b371f23b729ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5eb21ebc5713d43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Improve",
   "id": "7919f8f729522d4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import neptune\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from rouge_score import rouge_scorer\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from neptune.types import File\n",
    "\n",
    "# Inisialisasi Neptune\n",
    "run = neptune.init_run(\n",
    "    project=\"gabrielbatavia/Pintarpath\",\n",
    "    api_token=os.getenv(\"NEPTUNE_API_TOKEN\"),  # Gunakan environment variable untuk keamanan\n",
    "    name=\"Fine-tuning Nusantara Chat Model\",\n",
    "    tags=[\"fine-tuning\", \"NLP\", \"Transformers\"],\n",
    "    dependencies=\"infer\",\n",
    "    capture_hardware_metrics=True,\n",
    ")\n",
    "\n",
    "# Fungsi untuk mendownload dataset dari bucket GCS\n",
    "def download_from_gcs(bucket_path, local_path):\n",
    "    subprocess.run(['gsutil', 'cp', bucket_path, local_path])\n",
    "\n",
    "# Fungsi untuk mengupload file ke bucket GCS\n",
    "def upload_to_gcs(local_path, bucket_path):\n",
    "    subprocess.run(['gsutil', 'cp', local_path, bucket_path])\n",
    "\n",
    "# Menggunakan GPU jika tersedia, fallback ke CPU jika OutOfMemoryError terjadi\n",
    "def load_model(model_name):\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(\"Model loaded on GPU\" if torch.cuda.is_available() else \"Model loaded on CPU\")\n",
    "    except RuntimeError as e:\n",
    "        if 'out of memory' in str(e):\n",
    "            print(\"Out of memory error on GPU. Loading model on CPU...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name).to('cpu')\n",
    "        else:\n",
    "            raise e\n",
    "    return model\n",
    "\n",
    "# Load dataset dari file JSON\n",
    "def load_dataset(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Fungsi evaluasi ROUGE dan F1 Score\n",
    "def compute_metrics(pred):\n",
    "    predictions = pred.predictions.argmax(-1)\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    # Decode predictions dan labels menjadi teks\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Hitung F1 Score\n",
    "    f1 = f1_score(decoded_labels, decoded_preds, average='weighted')\n",
    "\n",
    "    # Hitung ROUGE\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {key: 0 for key in scorer.score(\"\", \"\").keys()}\n",
    "    n = len(decoded_preds)\n",
    "\n",
    "    for i in range(n):\n",
    "        scores = scorer.score(decoded_labels[i], decoded_preds[i])\n",
    "        for key in rouge_scores:\n",
    "            rouge_scores[key] += scores[key].fmeasure\n",
    "\n",
    "    # Ambil rata-rata ROUGE score\n",
    "    rouge_scores = {key: value / n for key, value in rouge_scores.items()}\n",
    "\n",
    "    # Log metrik ke Neptune secara eksplisit\n",
    "    run[\"metrics/accuracy\"].append(accuracy_score(labels, predictions))\n",
    "    run[\"metrics/f1\"].append(f1)\n",
    "    run[\"metrics/rouge1\"].append(rouge_scores['rouge1'])\n",
    "    run[\"metrics/rouge2\"].append(rouge_scores['rouge2'])\n",
    "    run[\"metrics/rougeL\"].append(rouge_scores['rougeL'])\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1,\n",
    "        \"rouge1\": rouge_scores['rouge1'],\n",
    "        \"rouge2\": rouge_scores['rouge2'],\n",
    "        \"rougeL\": rouge_scores['rougeL']\n",
    "    }\n",
    "\n",
    "# Modifikasi fungsi fine_tune_model untuk menyertakan tokenizer sebagai global variable\n",
    "def fine_tune_model(model_name, dataset_file, output_dir):\n",
    "    global tokenizer\n",
    "    # Load model dan tokenizer\n",
    "    model = load_model(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Load dataset\n",
    "    data = load_dataset(dataset_file)\n",
    "\n",
    "    # Preprocess dataset\n",
    "    inputs = tokenizer([conv['conversations'][0]['value'] for conv in data], return_tensors='pt', truncation=True, padding='max_length', max_length=64)\n",
    "    labels = tokenizer([conv['conversations'][1]['value'] for conv in data], return_tensors='pt', truncation=True, padding='max_length', max_length=64)\n",
    "\n",
    "    class CustomDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "            item['labels'] = self.labels['input_ids'][idx]\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels['input_ids'])\n",
    "\n",
    "    dataset = CustomDataset(inputs, labels)\n",
    "\n",
    "    # Logging hyperparameter ke Neptune\n",
    "    params = {\n",
    "        \"model_name\": model_name,\n",
    "        \"batch_size\": 8,\n",
    "        \"num_train_epochs\": 100,\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"gradient_accumulation_steps\": 16,\n",
    "    }\n",
    "    run[\"parameters\"] = params\n",
    "\n",
    "    # Mendefinisikan argumen training\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=params[\"batch_size\"],\n",
    "        gradient_accumulation_steps=params[\"gradient_accumulation_steps\"],\n",
    "        num_train_epochs=params[\"num_train_epochs\"],\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=250,\n",
    "        logging_dir='./logs',\n",
    "        report_to=\"none\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "        compute_metrics=compute_metrics,  # Menggunakan fungsi compute_metrics yang baru\n",
    "    )\n",
    "\n",
    "    # Jalankan pelatihan\n",
    "    trainer.train()\n",
    "\n",
    "    # Contoh Logging Gambar Visualisasi ke Neptune dan GCS\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist([1, 2, 1], bins=3)  # Contoh histogram, ganti dengan visualisasi aktual\n",
    "    plt.title(\"Contoh Visualisasi Histogram\")\n",
    "    \n",
    "    # Simpan gambar lokal\n",
    "    local_image_path = \"histogram.png\"\n",
    "    plt.savefig(local_image_path)\n",
    "    \n",
    "    # Logging gambar ke Neptune\n",
    "    run[\"train/distribution\"].upload(local_image_path)\n",
    "    \n",
    "    # Upload gambar ke GCS\n",
    "    bucket_image_path = 'gs://pintarpath-trying/visualizations/histogram.png'\n",
    "    upload_to_gcs(local_image_path, bucket_image_path)\n",
    "\n",
    "    # Simpan model dan tokenizer\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Hentikan logging Neptune\n",
    "    run.stop()\n",
    "\n",
    "    print(f\"Model fine-tuning selesai dan disimpan di: {output_dir}\")\n",
    "\n",
    "# Contoh penggunaan\n",
    "bucket_dataset_path = 'gs://pintarpath-trying/output2.json'  # Path di GCS\n",
    "local_dataset_file = './output2.json'  # Path lokal untuk dataset\n",
    "# bucket_output_dir = 'gs://pintarpath-trying/saved_model/fine-tuned-model'  # Path output di GCS\n",
    "local_output_dir = './fine-tuned-model'  # Path lokal untuk menyimpan model sebelum upload\n",
    "\n",
    "# Download dataset dari GCS\n",
    "download_from_gcs(bucket_dataset_path, local_dataset_file)\n",
    "\n",
    "# Fine-tuning model\n",
    "fine_tune_model(model_name=\"kalisai/Nusantara-0.8b-Indo-Chat\", dataset_file=local_dataset_file, output_dir=local_output_dir)\n",
    "\n",
    "# Upload model ke GCS (Dikomentari untuk menghemat penyimpanan)\n",
    "# upload_to_gcs(local_output_dir, bucket_output_dir)\n"
   ],
   "id": "35dbed4a5b9aa2a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "472344c3a2dbd45c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import neptune\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from rouge_score import rouge_scorer\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Inisialisasi Neptune\n",
    "run = neptune.init_run(\n",
    "    project=\"gabrielbatavia/Pintarpath\",\n",
    "    api_token = \"\",\n",
    "    name=\"Fine-tuning Nusantara Chat Model\",\n",
    "    tags=[\"fine-tuning\", \"NLP\", \"Transformers\"],\n",
    "    dependencies=\"infer\",\n",
    "    capture_hardware_metrics=True,\n",
    ")\n",
    "\n",
    "# Fungsi untuk mendownload dataset dari bucket GCS\n",
    "def download_from_gcs(bucket_path, local_path):\n",
    "    subprocess.run(['gsutil', 'cp', bucket_path, local_path])\n",
    "\n",
    "# Fungsi untuk mengupload file ke bucket GCS\n",
    "def upload_to_gcs(local_path, bucket_path):\n",
    "    subprocess.run(['gsutil', 'cp', local_path, bucket_path])\n",
    "\n",
    "# Menggunakan GPU jika tersedia, fallback ke CPU jika OutOfMemoryError terjadi\n",
    "def load_model(model_name):\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(\"Model loaded on GPU\" if torch.cuda.is_available() else \"Model loaded on CPU\")\n",
    "    except RuntimeError as e:\n",
    "        if 'out of memory' in str(e):\n",
    "            print(\"Out of memory error on GPU. Loading model on CPU...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name).to('cpu')\n",
    "        else:\n",
    "            raise e\n",
    "    return model\n",
    "\n",
    "# Load dataset dari file JSON\n",
    "def load_dataset(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Fungsi evaluasi ROUGE dan F1 Score\n",
    "def compute_metrics(pred):\n",
    "    predictions = pred.predictions.argmax(-1)\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    # Decode predictions dan labels menjadi teks\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Hitung F1 Score\n",
    "    f1 = f1_score(decoded_labels, decoded_preds, average='weighted')\n",
    "\n",
    "    # Hitung ROUGE\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {key: 0 for key in scorer.score(\"\", \"\").keys()}\n",
    "    n = len(decoded_preds)\n",
    "\n",
    "    for i in range(n):\n",
    "        scores = scorer.score(decoded_labels[i], decoded_preds[i])\n",
    "        for key in rouge_scores:\n",
    "            rouge_scores[key] += scores[key].fmeasure\n",
    "\n",
    "    # Ambil rata-rata ROUGE score\n",
    "    rouge_scores = {key: value / n for key, value in rouge_scores.items()}\n",
    "\n",
    "    # Log metrik ke Neptune secara eksplisit\n",
    "    run[\"metrics/accuracy\"].append(accuracy_score(labels, predictions))\n",
    "    run[\"metrics/f1\"].append(f1)\n",
    "    run[\"metrics/rouge1\"].append(rouge_scores['rouge1'])\n",
    "    run[\"metrics/rouge2\"].append(rouge_scores['rouge2'])\n",
    "    run[\"metrics/rougeL\"].append(rouge_scores['rougeL'])\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1,\n",
    "        \"rouge1\": rouge_scores['rouge1'],\n",
    "        \"rouge2\": rouge_scores['rouge2'],\n",
    "        \"rougeL\": rouge_scores['rougeL']\n",
    "    }\n",
    "\n",
    "# Modifikasi fungsi fine_tune_model untuk menyertakan tokenizer sebagai global variable\n",
    "def fine_tune_model(model_name, dataset_file, output_dir):\n",
    "    global tokenizer\n",
    "    # Load model dan tokenizer\n",
    "    model = load_model(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Load dataset\n",
    "    data = load_dataset(dataset_file)\n",
    "\n",
    "    # Preprocess dataset\n",
    "    inputs = tokenizer([conv['conversations'][0]['value'] for conv in data], return_tensors='pt', truncation=True, padding='max_length', max_length=64)\n",
    "    labels = tokenizer([conv['conversations'][1]['value'] for conv in data], return_tensors='pt', truncation=True, padding='max_length', max_length=64)\n",
    "\n",
    "    class CustomDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "            item['labels'] = self.labels['input_ids'][idx]\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels['input_ids'])\n",
    "\n",
    "    dataset = CustomDataset(inputs, labels)\n",
    "\n",
    "    # Logging hyperparameter ke Neptune\n",
    "    params = {\n",
    "        \"model_name\": model_name,\n",
    "        \"batch_size\": 12,\n",
    "        \"num_train_epochs\": 20,\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"gradient_accumulation_steps\": 16,\n",
    "    }\n",
    "    run[\"parameters\"] = params\n",
    "\n",
    "    # Mendefinisikan argumen training\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=params[\"batch_size\"],\n",
    "        gradient_accumulation_steps=params[\"gradient_accumulation_steps\"],\n",
    "        num_train_epochs=params[\"num_train_epochs\"],\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=250,\n",
    "        logging_dir='./logs',\n",
    "        report_to=\"none\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "        compute_metrics=compute_metrics,  # Menggunakan fungsi compute_metrics yang baru\n",
    "    )\n",
    "\n",
    "    # Jalankan pelatihan\n",
    "    trainer.train()\n",
    "\n",
    "    # Contoh Logging Gambar Visualisasi ke Neptune dan GCS\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist([1, 2, 1], bins=3)  # Contoh histogram, ganti dengan visualisasi aktual\n",
    "    plt.title(\"Contoh Visualisasi Histogram\")\n",
    "    \n",
    "    # Simpan gambar lokal\n",
    "    local_image_path = \"histogram.png\"\n",
    "    plt.savefig(local_image_path)\n",
    "    \n",
    "    # Logging gambar ke Neptune\n",
    "    run[\"train/distribution\"].upload(local_image_path)\n",
    "    \n",
    "    # Upload gambar ke GCS\n",
    "    bucket_image_path = 'gs://pintarpath-trying/visualizations/histogram.png'\n",
    "    upload_to_gcs(local_image_path, bucket_image_path)\n",
    "\n",
    "    # Simpan model dan tokenizer\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Hentikan logging Neptune\n",
    "    run.stop()\n",
    "\n",
    "    print(f\"Model fine-tuning selesai dan disimpan di: {output_dir}\")\n",
    "\n",
    "# Contoh penggunaan\n",
    "bucket_dataset_path = 'gs://pintarpath-trying/output2.json'  # Path di GCS\n",
    "local_dataset_file = './output2.json'  # Path lokal untuk dataset\n",
    "local_output_dir = './fine-tuned-model'  # Path lokal untuk menyimpan model sebelum upload\n",
    "\n",
    "# Download dataset dari GCS\n",
    "download_from_gcs(bucket_dataset_path, local_dataset_file)\n",
    "\n",
    "# Fine-tuning model\n",
    "fine_tune_model(model_name=\"kalisai/Nusantara-0.8b-Indo-Chat\", dataset_file=local_dataset_file, output_dir=local_output_dir)\n",
    "\n"
   ],
   "id": "6dccf36572f6c6ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
