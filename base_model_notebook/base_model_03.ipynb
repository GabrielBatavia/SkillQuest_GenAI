{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Library and Utility Function",
   "id": "41165d17bfd942b6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-05T17:44:24.503536Z",
     "start_time": "2024-08-05T17:44:21.615031Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Fungsi untuk memuat kata-kata dari file\n",
    "def load_words(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        words = file.read().splitlines()\n",
    "    return words\n",
    "\n",
    "# Fungsi untuk memuat kata-kata slang\n",
    "def load_slang_words(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        slang_dict = json.loads(file.read())\n",
    "    return slang_dict\n",
    "\n",
    "# Fungsi untuk membaca file teks dari folder\n",
    "def read_text_files(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read().strip()\n",
    "                if content:  # Check if the file is not empty\n",
    "                    cleaned_content = clean_text(content)\n",
    "                    texts.append(cleaned_content)\n",
    "    return texts\n",
    "\n",
    "# Fungsi untuk membersihkan teks\n",
    "def clean_text(text):\n",
    "    unwanted_chars = ['*', '#', '_', ')', '(', '!', '?', '.', ',', '-']\n",
    "    for char in unwanted_chars:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "d5d32cbf92f399b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T17:44:24.706506Z",
     "start_time": "2024-08-05T17:44:24.506048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Memuat data untuk dataset 1 (kosakata)\n",
    "root_words = load_words('../Dataset/indonesian_word/combined_root_words.txt')\n",
    "stop_words = load_words('../Dataset/indonesian_word/combined_stop_words.txt')\n",
    "slang_dict = load_slang_words('../Dataset/indonesian_word/combined_slang_words.txt')\n",
    "\n",
    "# Gabungkan semua kata untuk pembentukan kosakata\n",
    "all_words = list(set(root_words + stop_words + list(slang_dict.keys()) + list(slang_dict.values())))\n",
    "\n",
    "# Memuat dataset 2 (paragraf tentang Python)\n",
    "dataset2_texts = read_text_files('../Dataset/nlp_dataset')\n"
   ],
   "id": "ce28774032ee8649",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configure GPU",
   "id": "2cde5cc88bfa56ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T17:44:26.391089Z",
     "start_time": "2024-08-05T17:44:24.707977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure TensorFlow to use a specific amount of GPU memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])  # Set memory limit to 2GB\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Enable mixed precision training\n",
    "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
    "\n",
    "# Set mixed precision policy to use float16\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)\n"
   ],
   "id": "53ad9abc0ad47721",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Prepeare",
   "id": "5c13981cd92520ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T17:44:27.282645Z",
     "start_time": "2024-08-05T17:44:26.392088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenizer untuk dataset 1\n",
    "tokenizer = Tokenizer(num_words=len(all_words), oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(all_words)\n",
    "\n",
    "# Tokenisasi dan padding untuk dataset 1\n",
    "dataset1_sequences = tokenizer.texts_to_sequences(all_words)\n",
    "dataset1_padded = pad_sequences(dataset1_sequences, padding='post')\n",
    "\n",
    "# Dataset dan target untuk pelatihan\n",
    "dataset1_inputs = dataset1_padded[:, :-1]\n",
    "dataset1_targets = dataset1_padded[:, 1:]\n",
    "\n",
    "# Create a dataset from the input and target sequences\n",
    "train_dataset1 = tf.data.Dataset.from_tensor_slices((dataset1_inputs, dataset1_targets))\n",
    "train_dataset1 = train_dataset1.shuffle(buffer_size=1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ],
   "id": "8c801a003441b96d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Arsitektur words model",
   "id": "5173a5c5511cfa33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T17:44:27.699987Z",
     "start_time": "2024-08-05T17:44:27.285063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 256\n",
    "lstm_units = 512\n",
    "vocab_size = len(all_words)  # Adjust based on your vocabulary size\n",
    "\n",
    "# Input Layer\n",
    "inputs = Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "# Embedding Layer\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"embedding_layer\")(inputs)\n",
    "\n",
    "# LSTM Layer\n",
    "lstm_output = LSTM(lstm_units, return_sequences=True, name=\"lstm_layer\")(embedding)\n",
    "\n",
    "# Output Layer for Vocabulary Understanding\n",
    "outputs = Dense(vocab_size, activation='softmax', name=\"output_layer\")(lstm_output)\n",
    "\n",
    "# Model\n",
    "vocab_model = Model(inputs=inputs, outputs=outputs, name=\"vocab_model\")\n",
    "\n",
    "# Compile Model with mixed precision optimizer\n",
    "optimizer = Adam()\n",
    "vocab_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model Summary\n",
    "vocab_model.summary()\n"
   ],
   "id": "f6a87582c77b40ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vocab_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_layer (Embedding)  (None, None, 256)        7588608   \n",
      "                                                                 \n",
      " lstm_layer (LSTM)           (None, None, 512)         1574912   \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, None, 29643)       15206859  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,370,379\n",
      "Trainable params: 24,370,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train Words model",
   "id": "2c1d6e551badb4ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T17:47:43.157657Z",
     "start_time": "2024-08-05T17:46:31.813737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accumulation_steps = 4  # Simulate batch size of 4 times larger\n",
    "\n",
    "@tf.function\n",
    "def train_step_vocabulary(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = vocab_model(inputs, training=True)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(targets, predictions)\n",
    "    scaled_loss = loss / accumulation_steps\n",
    "    gradients = tape.gradient(scaled_loss, vocab_model.trainable_variables)\n",
    "    return gradients, loss\n",
    "\n",
    "# Custom training loop for dataset 1\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    print(f'Epoch {epoch + 1}/{10}')\n",
    "    accum_gradients = [tf.zeros_like(var, dtype=tf.float32) for var in vocab_model.trainable_variables]\n",
    "    for step, (inputs, targets) in enumerate(train_dataset1):\n",
    "        gradients, loss = train_step_vocabulary(inputs, targets)\n",
    "        accum_gradients = [\n",
    "            accum_grad + (grad if grad is not None else tf.zeros_like(accum_grad))\n",
    "            for accum_grad, grad in zip(accum_gradients, gradients)\n",
    "        ]\n",
    "        \n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            optimizer.apply_gradients(zip(accum_gradients, vocab_model.trainable_variables))\n",
    "            accum_gradients = [tf.zeros_like(var, dtype=tf.float32) for var in vocab_model.trainable_variables]\n",
    "    \n",
    "    # Apply remaining gradients if the number of steps is not a multiple of accumulation_steps\n",
    "    if step % accumulation_steps != 0:\n",
    "        optimizer.apply_gradients(zip(accum_gradients, vocab_model.trainable_variables))\n",
    "    \n",
    "    print(f'Loss: {loss.numpy().mean()}')\n",
    "    \n",
    "# Save the embedding layer weights for future use\n",
    "vocab_model.save_weights('../saved_model/base_model_saved/base_model_03/vocab_model_weights.h5')\n",
    "\n"
   ],
   "id": "a633e8f143112cc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Loss: 10.296875\n",
      "Epoch 2/10\n",
      "Loss: 10.296875\n",
      "Epoch 3/10\n",
      "Loss: 10.296875\n",
      "Epoch 4/10\n",
      "Loss: 10.296875\n",
      "Epoch 5/10\n",
      "Loss: 10.296875\n",
      "Epoch 6/10\n",
      "Loss: 10.296875\n",
      "Epoch 7/10\n",
      "Loss: 10.296875\n",
      "Epoch 8/10\n",
      "Loss: 10.296875\n",
      "Epoch 9/10\n",
      "Loss: 10.296875\n",
      "Epoch 10/10\n",
      "Loss: 10.296875\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Arsitektur Model Attention",
   "id": "f056a1cbda92f98e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T17:48:11.720998Z",
     "start_time": "2024-08-05T17:48:10.939213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pretrained embedding weights\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"embedding_layer\")\n",
    "embedding_layer.build((None,))\n",
    "embedding_layer.set_weights(vocab_model.get_layer('embedding_layer').get_weights())\n",
    "\n",
    "# Transformer Layer Function\n",
    "def transformer_layer(x, d_model, num_heads, dff, rate=0.1):\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "    \n",
    "    ffn_output = tf.keras.Sequential([\n",
    "        Dense(dff, activation='relu'), \n",
    "        Dense(d_model)])(out1)\n",
    "    \n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "    \n",
    "    return out2\n",
    "\n",
    "# Parameters\n",
    "embedding_dim = 256\n",
    "num_transformer_layers = 4\n",
    "num_heads = 8\n",
    "dff = 1024\n",
    "\n",
    "# Model with attention layers\n",
    "inputs = Input(shape=(None,), name=\"inputs\")\n",
    "embedding = embedding_layer(inputs)\n",
    "\n",
    "transformer_output = embedding\n",
    "for _ in range(num_transformer_layers):\n",
    "    transformer_output = transformer_layer(transformer_output, embedding_dim, num_heads, dff)\n",
    "\n",
    "outputs = Dense(vocab_size, activation='softmax', name=\"output_layer\")(transformer_output)\n",
    "\n",
    "context_model = Model(inputs=inputs, outputs=outputs, name=\"context_model\")\n",
    "\n",
    "# Compile Model with mixed precision optimizer\n",
    "optimizer = Adam()\n",
    "context_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "context_model.summary()\n"
   ],
   "id": "7814bcb72612d009",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"context_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inputs (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_layer (Embedding)    (None, None, 256)    7588608     ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, None, 256)   2103552     ['embedding_layer[0][0]',        \n",
      " dAttention)                                                      'embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 256)    0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, None, 256)   0           ['embedding_layer[0][0]',        \n",
      " da)                                                              'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, None, 256)   512         ['tf.__operators__.add[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, None, 256)    525568      ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, None, 256)    0           ['sequential[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, None, 256)   0           ['layer_normalization[0][0]',    \n",
      " mbda)                                                            'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, None, 256)   512         ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, None, 256)   2103552     ['layer_normalization_1[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, None, 256)    0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, None, 256)   0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, None, 256)   512         ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, None, 256)    525568      ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, None, 256)    0           ['sequential_1[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, None, 256)   0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, None, 256)   512         ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, None, 256)   2103552     ['layer_normalization_3[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, None, 256)    0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, None, 256)   0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, None, 256)   512         ['tf.__operators__.add_4[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)      (None, None, 256)    525568      ['layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, None, 256)    0           ['sequential_2[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, None, 256)   0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, None, 256)   512         ['tf.__operators__.add_5[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, None, 256)   2103552     ['layer_normalization_5[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, None, 256)    0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, None, 256)   0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, None, 256)   512         ['tf.__operators__.add_6[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_3 (Sequential)      (None, None, 256)    525568      ['layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, None, 256)    0           ['sequential_3[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, None, 256)   0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, None, 256)   512         ['tf.__operators__.add_7[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, None, 29643)  7618251     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,727,435\n",
      "Trainable params: 25,727,435\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset 2 Prep",
   "id": "cd606a79b6c0bf4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T17:48:12.731914Z",
     "start_time": "2024-08-05T17:48:12.492251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenizer untuk dataset 2 (menggunakan tokenizer yang sama)\n",
    "dataset2_sequences = tokenizer.texts_to_sequences(dataset2_texts)\n",
    "dataset2_padded = pad_sequences(dataset2_sequences, padding='post')\n",
    "\n",
    "# Dataset dan target untuk pelatihan\n",
    "dataset2_inputs = dataset2_padded[:, :-1]\n",
    "dataset2_targets = dataset2_padded[:, 1:]\n",
    "\n",
    "# Create a dataset from the input and target sequences\n",
    "train_dataset2 = tf.data.Dataset.from_tensor_slices((dataset2_inputs, dataset2_targets))\n",
    "train_dataset2 = train_dataset2.shuffle(buffer_size=1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ],
   "id": "61644a885ef2476e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Model Attention",
   "id": "601b0ad7de65c1fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T17:48:24.914814Z",
     "start_time": "2024-08-05T17:48:13.547799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom training loop for dataset 2 with gradient accumulation\n",
    "accumulation_steps = 4  # Simulate batch size of 4 times larger\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = context_model(inputs, training=True)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(targets, predictions)\n",
    "    gradients = tape.gradient(loss, context_model.trainable_variables)\n",
    "    return gradients, loss\n",
    "\n",
    "# Custom training loop for dataset 2\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    print(f'Epoch {epoch + 1}/{10}')\n",
    "    accum_gradients = [tf.zeros_like(var) for var in context_model.trainable_variables]\n",
    "    for step, (inputs, targets) in enumerate(train_dataset2):\n",
    "        gradients, loss = train_step(inputs, targets)\n",
    "        accum_gradients = [accum_grad + grad for accum_grad, grad in zip(accum_gradients, gradients)]\n",
    "        \n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            context_model.optimizer.apply_gradients(zip(accum_gradients, context_model.trainable_variables))\n",
    "            accum_gradients = [tf.zeros_like(var) for var in context_model.trainable_variables]\n",
    "    \n",
    "    # Apply remaining gradients if the number of steps is not a multiple of accumulation_steps\n",
    "    if step % accumulation_steps != 0:\n",
    "        context_model.optimizer.apply_gradients(zip(accum_gradients, context_model.trainable_variables))\n",
    "    \n",
    "    print(f'Loss: {loss.numpy().mean()}')\n"
   ],
   "id": "1b9e0c75927ec290",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'context_model/multi_head_attention/value/einsum/Einsum' defined at (most recent call last):\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_30180\\4093364320.py\", line 17, in <module>\n      gradients, loss = train_step(inputs, targets)\n    File \"C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_30180\\4093364320.py\", line 7, in train_step\n      predictions = context_model(inputs, training=True)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\layers\\multi_head_attention.py\", line 508, in call\n      value = self._value_dense(value)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\layers\\einsum_dense.py\", line 187, in call\n      ret = tf.einsum(self.equation, inputs, self.kernel)\nNode: 'context_model/multi_head_attention/value/einsum/Einsum'\nOOM when allocating tensor with shape[126464,2048] and type half on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node context_model/multi_head_attention/value/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_step_243878]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 17\u001B[0m\n\u001B[0;32m     15\u001B[0m accum_gradients \u001B[38;5;241m=\u001B[39m [tf\u001B[38;5;241m.\u001B[39mzeros_like(var) \u001B[38;5;28;01mfor\u001B[39;00m var \u001B[38;5;129;01min\u001B[39;00m context_model\u001B[38;5;241m.\u001B[39mtrainable_variables]\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, (inputs, targets) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_dataset2):\n\u001B[1;32m---> 17\u001B[0m     gradients, loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m     accum_gradients \u001B[38;5;241m=\u001B[39m [accum_grad \u001B[38;5;241m+\u001B[39m grad \u001B[38;5;28;01mfor\u001B[39;00m accum_grad, grad \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(accum_gradients, gradients)]\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m accumulation_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[0;32m     55\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mResourceExhaustedError\u001B[0m: Graph execution error:\n\nDetected at node 'context_model/multi_head_attention/value/einsum/Einsum' defined at (most recent call last):\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_30180\\4093364320.py\", line 17, in <module>\n      gradients, loss = train_step(inputs, targets)\n    File \"C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_30180\\4093364320.py\", line 7, in train_step\n      predictions = context_model(inputs, training=True)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\layers\\multi_head_attention.py\", line 508, in call\n      value = self._value_dense(value)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\layers\\einsum_dense.py\", line 187, in call\n      ret = tf.einsum(self.equation, inputs, self.kernel)\nNode: 'context_model/multi_head_attention/value/einsum/Einsum'\nOOM when allocating tensor with shape[126464,2048] and type half on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node context_model/multi_head_attention/value/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_step_243878]"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4834e6bd85b852c0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
