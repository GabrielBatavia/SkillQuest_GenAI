{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Library and Utility Function",
   "id": "41165d17bfd942b6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-07T20:02:58.245433Z",
     "start_time": "2024-08-07T20:02:54.898335Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Fungsi untuk memuat kata-kata dari file\n",
    "def load_words(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        words = file.read().splitlines()\n",
    "    return words\n",
    "\n",
    "# Fungsi untuk memuat kata-kata slang\n",
    "def load_slang_words(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        slang_dict = json.loads(file.read())\n",
    "    return slang_dict\n",
    "\n",
    "# Fungsi untuk membaca file teks dari folder\n",
    "def read_text_files(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read().strip()\n",
    "                if content:  # Check if the file is not empty\n",
    "                    cleaned_content = clean_text(content)\n",
    "                    texts.append(cleaned_content)\n",
    "    return texts\n",
    "\n",
    "# Fungsi untuk membaca file sampling\n",
    "def read_sampling_files(folder_path, num_files=100):\n",
    "    texts = []\n",
    "    filenames = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "    sampled_filenames = filenames[:num_files]  # Ambil hanya sejumlah file yang diperlukan\n",
    "    for filename in sampled_filenames:\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            content = file.read().strip()\n",
    "            if content:  # Check if the file is not empty\n",
    "                cleaned_content = clean_text(content)\n",
    "                texts.append(cleaned_content)\n",
    "    return texts\n",
    "\n",
    "# Fungsi untuk membersihkan teks\n",
    "def clean_text(text):\n",
    "    unwanted_chars = ['*', '#', '_', ')', '(', '!', '?', '.', ',', '-']\n",
    "    for char in unwanted_chars:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "d5d32cbf92f399b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T20:02:58.796915Z",
     "start_time": "2024-08-07T20:02:58.247605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Memuat data untuk dataset 1 (kosakata)\n",
    "root_words = load_words('../Dataset/indonesian_word/combined_root_words.txt')\n",
    "stop_words = load_words('../Dataset/indonesian_word/combined_stop_words.txt')\n",
    "slang_dict = load_slang_words('../Dataset/indonesian_word/combined_slang_words.txt')\n",
    "\n",
    "# Gabungkan semua kata untuk pembentukan kosakata\n",
    "all_words = list(set(root_words + stop_words + list(slang_dict.keys()) + list(slang_dict.values())))\n",
    "\n",
    "# Memuat dataset 2 (paragraf tentang Python)\n",
    "dataset2_texts = read_text_files('../Dataset/nlp_dataset')\n",
    "\n",
    "# Memuat dataset 2 dengan metode sampling\n",
    "dataset2_sampling = read_sampling_files('../Dataset/nlp_dataset', num_files=1)\n"
   ],
   "id": "ce28774032ee8649",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T20:02:58.858801Z",
     "start_time": "2024-08-07T20:02:58.798793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Menghitung dan mencetak jumlah kata dalam dataset 2\n",
    "total_words_dataset2 = sum(len(text.split()) for text in dataset2_texts)\n",
    "print(f'Total jumlah kata dalam dataset 2: {total_words_dataset2}')\n"
   ],
   "id": "62fc947c686b86a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total jumlah kata dalam dataset 2: 1020965\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T17:56:53.072282Z",
     "start_time": "2024-08-06T17:56:53.064850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# mencetak jumlah kata dalam sampling\n",
    "total_words_dataset_sampling = sum(len(text.split()) for text in dataset2_sampling)\n",
    "print(f'Total jumlah kata dalam dataset sampling: {total_words_dataset_sampling}')"
   ],
   "id": "8fa9eceb560e7e69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total jumlah kata dalam dataset sampling: 1039\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configure GPU",
   "id": "2cde5cc88bfa56ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T17:56:54.353820Z",
     "start_time": "2024-08-06T17:56:54.336162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure TensorFlow to use a specific amount of GPU memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])  # Set memory limit to 2GB\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Enable mixed precision training\n",
    "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
    "\n",
    "# Set mixed precision policy to use float16\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)\n"
   ],
   "id": "53ad9abc0ad47721",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Prepeare",
   "id": "5c13981cd92520ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T17:56:55.112529Z",
     "start_time": "2024-08-06T17:56:54.753557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenizer untuk dataset 1\n",
    "tokenizer = Tokenizer(num_words=len(all_words), oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(all_words)\n",
    "\n",
    "# Tokenisasi dan padding untuk dataset 1\n",
    "dataset1_sequences = tokenizer.texts_to_sequences(all_words)\n",
    "dataset1_padded = pad_sequences(dataset1_sequences, padding='post')\n",
    "\n",
    "# Dataset dan target untuk pelatihan\n",
    "dataset1_inputs = dataset1_padded[:, :-1]\n",
    "dataset1_targets = dataset1_padded[:, 1:]\n",
    "\n",
    "# Create a dataset from the input and target sequences\n",
    "train_dataset1 = tf.data.Dataset.from_tensor_slices((dataset1_inputs, dataset1_targets))\n",
    "train_dataset1 = train_dataset1.shuffle(buffer_size=1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ],
   "id": "8c801a003441b96d",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Arsitektur words model",
   "id": "5173a5c5511cfa33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T17:56:55.406099Z",
     "start_time": "2024-08-06T17:56:55.176270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 64\n",
    "lstm_units = 128\n",
    "vocab_size = len(all_words)  # Adjust based on your vocabulary size\n",
    "\n",
    "# Input Layer\n",
    "inputs = Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "# Embedding Layer\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"embedding_layer\")(inputs)\n",
    "\n",
    "# LSTM Layer\n",
    "lstm_output = LSTM(lstm_units, return_sequences=True, name=\"lstm_layer\")(embedding)\n",
    "\n",
    "# Output Layer for Vocabulary Understanding\n",
    "outputs = Dense(vocab_size, activation='softmax', name=\"output_layer\")(lstm_output)\n",
    "\n",
    "# Model\n",
    "vocab_model = Model(inputs=inputs, outputs=outputs, name=\"vocab_model\")\n",
    "\n",
    "# Compile Model with mixed precision optimizer\n",
    "optimizer = Adam()\n",
    "vocab_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model Summary\n",
    "vocab_model.summary()\n"
   ],
   "id": "f6a87582c77b40ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vocab_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_layer (Embedding)  (None, None, 64)         1897152   \n",
      "                                                                 \n",
      " lstm_layer (LSTM)           (None, None, 128)         98816     \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, None, 29643)       3823947   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,819,915\n",
      "Trainable params: 5,819,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train Words model",
   "id": "2c1d6e551badb4ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T17:57:14.696732Z",
     "start_time": "2024-08-06T17:56:55.567525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accumulation_steps = 4  # Simulate batch size of 4 times larger\n",
    "\n",
    "@tf.function\n",
    "def train_step_vocabulary(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = vocab_model(inputs, training=True)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(targets, predictions)\n",
    "    scaled_loss = loss / accumulation_steps\n",
    "    gradients = tape.gradient(scaled_loss, vocab_model.trainable_variables)\n",
    "    return gradients, loss\n",
    "\n",
    "# Custom training loop for dataset 1\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    print(f'Epoch {epoch + 1}/{10}')\n",
    "    accum_gradients = [tf.zeros_like(var, dtype=tf.float32) for var in vocab_model.trainable_variables]\n",
    "    for step, (inputs, targets) in enumerate(train_dataset1):\n",
    "        gradients, loss = train_step_vocabulary(inputs, targets)\n",
    "        accum_gradients = [\n",
    "            accum_grad + (grad if grad is not None else tf.zeros_like(accum_grad))\n",
    "            for accum_grad, grad in zip(accum_gradients, gradients)\n",
    "        ]\n",
    "        \n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            optimizer.apply_gradients(zip(accum_gradients, vocab_model.trainable_variables))\n",
    "            accum_gradients = [tf.zeros_like(var, dtype=tf.float32) for var in vocab_model.trainable_variables]\n",
    "    \n",
    "    # Apply remaining gradients if the number of steps is not a multiple of accumulation_steps\n",
    "    if step % accumulation_steps != 0:\n",
    "        optimizer.apply_gradients(zip(accum_gradients, vocab_model.trainable_variables))\n",
    "    \n",
    "    print(f'Loss: {loss.numpy().mean()}')\n",
    "    \n",
    "# Save the embedding layer weights for future use\n",
    "vocab_model.save_weights('../saved_model/base_model_saved/base_model_03/vocab_model_weights.h5')\n",
    "\n"
   ],
   "id": "a633e8f143112cc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Loss: 10.296875\n",
      "Epoch 2/10\n",
      "Loss: 10.296875\n",
      "Epoch 3/10\n",
      "Loss: 10.296875\n",
      "Epoch 4/10\n",
      "Loss: 10.296875\n",
      "Epoch 5/10\n",
      "Loss: 10.296875\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Arsitektur Model Attention",
   "id": "f056a1cbda92f98e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T17:57:15.303Z",
     "start_time": "2024-08-06T17:57:14.699734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pretrained embedding weights\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"embedding_layer\")\n",
    "embedding_layer.build((None,))\n",
    "embedding_layer.set_weights(vocab_model.get_layer('embedding_layer').get_weights())\n",
    "\n",
    "# Transformer Layer Function\n",
    "def transformer_layer(x, d_model, num_heads, dff, rate=0.1):\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "    \n",
    "    ffn_output = tf.keras.Sequential([\n",
    "        Dense(dff, activation='relu'), \n",
    "        Dense(d_model)])(out1)\n",
    "    \n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "    \n",
    "    return out2\n",
    "\n",
    "# Parameters\n",
    "embedding_dim = 64\n",
    "num_transformer_layers = 4\n",
    "num_heads = 8\n",
    "dff = 1024\n",
    "\n",
    "# Model with attention layers\n",
    "inputs = Input(shape=(None,), name=\"inputs\")\n",
    "embedding = embedding_layer(inputs)\n",
    "\n",
    "transformer_output = embedding\n",
    "for _ in range(num_transformer_layers):\n",
    "    transformer_output = transformer_layer(transformer_output, embedding_dim, num_heads, dff)\n",
    "\n",
    "outputs = Dense(vocab_size, activation='softmax', name=\"output_layer\")(transformer_output)\n",
    "\n",
    "context_model = Model(inputs=inputs, outputs=outputs, name=\"context_model\")\n",
    "\n",
    "# Compile Model with mixed precision optimizer\n",
    "optimizer = Adam()\n",
    "context_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "context_model.summary()\n"
   ],
   "id": "7814bcb72612d009",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"context_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inputs (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_layer (Embedding)    (None, None, 64)     1897152     ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (MultiH  (None, None, 64)    132672      ['embedding_layer[0][0]',        \n",
      " eadAttention)                                                    'embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, None, 64)     0           ['multi_head_attention_8[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (TFOpL  (None, None, 64)    0           ['embedding_layer[0][0]',        \n",
      " ambda)                                                           'dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, None, 64)    128         ['tf.__operators__.add_16[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_8 (Sequential)      (None, None, 64)     132160      ['layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, None, 64)     0           ['sequential_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (TFOpL  (None, None, 64)    0           ['layer_normalization_16[0][0]', \n",
      " ambda)                                                           'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, None, 64)    128         ['tf.__operators__.add_17[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, None, 64)    132672      ['layer_normalization_17[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, None, 64)     0           ['multi_head_attention_9[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (TFOpL  (None, None, 64)    0           ['layer_normalization_17[0][0]', \n",
      " ambda)                                                           'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_18 (LayerN  (None, None, 64)    128         ['tf.__operators__.add_18[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_9 (Sequential)      (None, None, 64)     132160      ['layer_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, None, 64)     0           ['sequential_9[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (TFOpL  (None, None, 64)    0           ['layer_normalization_18[0][0]', \n",
      " ambda)                                                           'dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_19 (LayerN  (None, None, 64)    128         ['tf.__operators__.add_19[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, None, 64)    132672      ['layer_normalization_19[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, None, 64)     0           ['multi_head_attention_10[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (TFOpL  (None, None, 64)    0           ['layer_normalization_19[0][0]', \n",
      " ambda)                                                           'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, None, 64)    128         ['tf.__operators__.add_20[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_10 (Sequential)     (None, None, 64)     132160      ['layer_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, None, 64)     0           ['sequential_10[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (TFOpL  (None, None, 64)    0           ['layer_normalization_20[0][0]', \n",
      " ambda)                                                           'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_21 (LayerN  (None, None, 64)    128         ['tf.__operators__.add_21[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (Multi  (None, None, 64)    132672      ['layer_normalization_21[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, None, 64)     0           ['multi_head_attention_11[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (TFOpL  (None, None, 64)    0           ['layer_normalization_21[0][0]', \n",
      " ambda)                                                           'dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_22 (LayerN  (None, None, 64)    128         ['tf.__operators__.add_22[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_11 (Sequential)     (None, None, 64)     132160      ['layer_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, None, 64)     0           ['sequential_11[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (TFOpL  (None, None, 64)    0           ['layer_normalization_22[0][0]', \n",
      " ambda)                                                           'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_23 (LayerN  (None, None, 64)    128         ['tf.__operators__.add_23[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, None, 29643)  1926795     ['layer_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,884,299\n",
      "Trainable params: 4,884,299\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset 2 Prep",
   "id": "cd606a79b6c0bf4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T17:57:15.318002Z",
     "start_time": "2024-08-06T17:57:15.304999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenizer untuk dataset 2 (menggunakan tokenizer yang sama)\n",
    "dataset2_sequences = tokenizer.texts_to_sequences(dataset2_sampling)\n",
    "dataset2_padded = pad_sequences(dataset2_sequences, padding='post')\n",
    "\n",
    "# Dataset dan target untuk pelatihan\n",
    "dataset2_inputs = dataset2_padded[:, :-1]\n",
    "dataset2_targets = dataset2_padded[:, 1:]\n",
    "\n",
    "# Create a dataset from the input and target sequences\n",
    "train_dataset2 = tf.data.Dataset.from_tensor_slices((dataset2_inputs, dataset2_targets))\n",
    "train_dataset2 = train_dataset2.shuffle(buffer_size=1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ],
   "id": "61644a885ef2476e",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Model Attention",
   "id": "601b0ad7de65c1fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T17:57:16.699525Z",
     "start_time": "2024-08-06T17:57:15.320001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom training loop for dataset 2 with gradient accumulation\n",
    "accumulation_steps = 4  # Simulate batch size of 4 times larger\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = context_model(inputs, training=True)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(targets, predictions)\n",
    "    gradients = tape.gradient(loss, context_model.trainable_variables)\n",
    "    return gradients, loss\n",
    "\n",
    "# Custom training loop for dataset 2\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    print(f'Epoch {epoch + 1}/{10}')\n",
    "    accum_gradients = [tf.zeros_like(var) for var in context_model.trainable_variables]\n",
    "    for step, (inputs, targets) in enumerate(train_dataset2):\n",
    "        gradients, loss = train_step(inputs, targets)\n",
    "        accum_gradients = [accum_grad + grad for accum_grad, grad in zip(accum_gradients, gradients)]\n",
    "        \n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            context_model.optimizer.apply_gradients(zip(accum_gradients, context_model.trainable_variables))\n",
    "            accum_gradients = [tf.zeros_like(var) for var in context_model.trainable_variables]\n",
    "    \n",
    "    # Apply remaining gradients if the number of steps is not a multiple of accumulation_steps\n",
    "    if step % accumulation_steps != 0:\n",
    "        context_model.optimizer.apply_gradients(zip(accum_gradients, context_model.trainable_variables))\n",
    "    \n",
    "    print(f'Loss: {loss.numpy().mean()}')\n"
   ],
   "id": "1b9e0c75927ec290",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Loss: 10.296875\n",
      "Epoch 2/10\n",
      "Loss: 10.296875\n",
      "Epoch 3/10\n",
      "Loss: 10.2890625\n",
      "Epoch 4/10\n",
      "Loss: 10.296875\n",
      "Epoch 5/10\n",
      "Loss: 10.296875\n",
      "Epoch 6/10\n",
      "Loss: 10.296875\n",
      "Epoch 7/10\n",
      "Loss: 10.2890625\n",
      "Epoch 8/10\n",
      "Loss: 10.296875\n",
      "Epoch 9/10\n",
      "Loss: 10.296875\n",
      "Epoch 10/10\n",
      "Loss: 10.296875\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "857b415a011af51f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
