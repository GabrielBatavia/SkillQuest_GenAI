{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Library and Utility Function\n",
   "id": "72145cd042c5bbca"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-06T19:41:13.376480Z",
     "start_time": "2024-08-06T19:41:13.352900Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Embedding, Dense, Input, MultiHeadAttention, LayerNormalization, Dropout, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "# Fungsi untuk memuat kata-kata dari file\n",
    "def load_words(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        words = file.read().splitlines()\n",
    "    return words\n",
    "\n",
    "# Fungsi untuk memuat kata-kata slang\n",
    "def load_slang_words(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        slang_dict = json.loads(file.read())\n",
    "    return slang_dict\n",
    "\n",
    "# Fungsi untuk membaca file teks dari folder\n",
    "def read_text_files(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read().strip()\n",
    "                if content:  # Check if the file is not empty\n",
    "                    cleaned_content = clean_text(content)\n",
    "                    texts.append(cleaned_content)\n",
    "    return texts\n",
    "\n",
    "# Fungsi untuk membaca file sampling\n",
    "def read_sampling_files(folder_path, num_files=100):\n",
    "    texts = []\n",
    "    filenames = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "    sampled_filenames = filenames[:num_files]  # Ambil hanya sejumlah file yang diperlukan\n",
    "    for filename in sampled_filenames:\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            content = file.read().strip()\n",
    "            if content:  # Check if the file is not empty\n",
    "                cleaned_content = clean_text(content)\n",
    "                texts.append(cleaned_content)\n",
    "    return texts\n",
    "\n",
    "\n",
    "# Fungsi untuk membersihkan teks\n",
    "def clean_text(text):\n",
    "    unwanted_chars = ['*', '#', '_', ')', '(', '!', '?', '.', ',', '-']\n",
    "    for char in unwanted_chars:\n",
    "        text = text.replace(char, '')\n",
    "    return text"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configure GPU",
   "id": "f449cc1d7746f653"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:36:08.130584Z",
     "start_time": "2024-08-06T19:36:08.110582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Konfigurasi Memori GPU dan Mixed Precision Training\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])  # Set memory limit to 2GB\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)"
   ],
   "id": "212bcf937443d469",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "9c4693496e7a3be4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:36:08.616417Z",
     "start_time": "2024-08-06T19:36:08.461516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Memuat data untuk dataset 1 (kosakata)\n",
    "root_words = load_words('../Dataset/indonesian_word/combined_root_words.txt')\n",
    "stop_words = load_words('../Dataset/indonesian_word/combined_stop_words.txt')\n",
    "slang_dict = load_slang_words('../Dataset/indonesian_word/combined_slang_words.txt')\n",
    "\n",
    "# Gabungkan semua kata untuk pembentukan kosakata\n",
    "all_words = list(set(root_words + stop_words + list(slang_dict.keys()) + list(slang_dict.values())))\n",
    "\n",
    "# Memuat dataset 2 (paragraf tentang Python)\n",
    "dataset2_texts = read_text_files('../Dataset/nlp_dataset')\n",
    "\n",
    "# Memuat dataset 2 dengan metode sampling\n",
    "dataset2_sampling = read_sampling_files('../Dataset/nlp_dataset', num_files=1)\n"
   ],
   "id": "919fb5e500d74907",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:36:08.771094Z",
     "start_time": "2024-08-06T19:36:08.719653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Menghitung dan mencetak jumlah kata dalam dataset 2\n",
    "total_words_dataset2 = sum(len(text.split()) for text in dataset2_texts)\n",
    "print(f'Total jumlah kata dalam dataset 2: {total_words_dataset2}')\n"
   ],
   "id": "ea5b0d6e0a77cf00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total jumlah kata dalam dataset 2: 798006\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:36:09.179253Z",
     "start_time": "2024-08-06T19:36:09.168257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# mencetak jumlah kata dalam sampling\n",
    "total_words_dataset_sampling = sum(len(text.split()) for text in dataset2_sampling)\n",
    "print(f'Total jumlah kata dalam dataset sampling: {total_words_dataset_sampling}')"
   ],
   "id": "da62729f399f1f8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total jumlah kata dalam dataset sampling: 1039\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Prepeare",
   "id": "4d22657e5e1c10b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:36:10.407097Z",
     "start_time": "2024-08-06T19:36:09.922945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenizer untuk dataset 1\n",
    "tokenizer = Tokenizer(num_words=len(all_words), oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(all_words)\n",
    "\n",
    "# Tokenisasi dan padding untuk dataset 1\n",
    "dataset1_sequences = tokenizer.texts_to_sequences(all_words)\n",
    "dataset1_padded = pad_sequences(dataset1_sequences, padding='post')\n",
    "\n",
    "# Dataset dan target untuk pelatihan\n",
    "dataset1_inputs = dataset1_padded[:, :-1]\n",
    "dataset1_targets = dataset1_padded[:, 1:]\n",
    "\n",
    "# Create a dataset from the input and target sequences\n",
    "train_dataset1 = tf.data.Dataset.from_tensor_slices((dataset1_inputs, dataset1_targets))\n",
    "train_dataset1 = train_dataset1.shuffle(buffer_size=1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ],
   "id": "da4a9e1c146dc684",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Arsitektur words model\n",
   "id": "c63695ea6a9c1de3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:36:11.301712Z",
     "start_time": "2024-08-06T19:36:11.070906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 64\n",
    "lstm_units = 128\n",
    "vocab_size = len(all_words)  # Adjust based on your vocabulary size\n",
    "\n",
    "# Input Layer\n",
    "inputs = Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "# Embedding Layer\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"embedding_layer\")(inputs)\n",
    "\n",
    "# LSTM Layer\n",
    "lstm_output = LSTM(lstm_units, return_sequences=True, name=\"lstm_layer\")(embedding)\n",
    "\n",
    "# Output Layer for Vocabulary Understanding\n",
    "outputs = Dense(vocab_size, activation='softmax', name=\"output_layer\")(lstm_output)\n",
    "\n",
    "# Model\n",
    "vocab_model = Model(inputs=inputs, outputs=outputs, name=\"vocab_model\")\n",
    "\n",
    "# Compile Model with mixed precision optimizer\n",
    "optimizer = Adam()\n",
    "vocab_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model Summary\n",
    "vocab_model.summary()\n"
   ],
   "id": "14a7a7c08be1f491",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vocab_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_layer (Embedding)  (None, None, 64)         1897152   \n",
      "                                                                 \n",
      " lstm_layer (LSTM)           (None, None, 128)         98816     \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, None, 29643)       3823947   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,819,915\n",
      "Trainable params: 5,819,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train Words model",
   "id": "cf6396c8f8607586"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:37:01.183685Z",
     "start_time": "2024-08-06T19:36:16.859321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pelatihan Model Kosakata dengan Gradient Accumulation\n",
    "accumulation_steps = 4\n",
    "\n",
    "@tf.function\n",
    "def train_step_vocabulary(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = vocab_model(inputs, training=True)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(targets, predictions)\n",
    "    scaled_loss = loss / accumulation_steps\n",
    "    gradients = tape.gradient(scaled_loss, vocab_model.trainable_variables)\n",
    "    return gradients, loss\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(f'Epoch {epoch + 1}/{10}')\n",
    "    accum_gradients = [tf.zeros_like(var, dtype=tf.float32) for var in vocab_model.trainable_variables]\n",
    "    for step, (inputs, targets) in enumerate(train_dataset1):\n",
    "        gradients, loss = train_step_vocabulary(inputs, targets)\n",
    "        accum_gradients = [\n",
    "            accum_grad + (grad if grad is not None else tf.zeros_like(accum_grad))\n",
    "            for accum_grad, grad in zip(accum_gradients, gradients)\n",
    "        ]\n",
    "        \n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            optimizer.apply_gradients(zip(accum_gradients, vocab_model.trainable_variables))\n",
    "            accum_gradients = [tf.zeros_like(var, dtype=tf.float32) for var in vocab_model.trainable_variables]\n",
    "    \n",
    "    if step % accumulation_steps != 0:\n",
    "        optimizer.apply_gradients(zip(accum_gradients, vocab_model.trainable_variables))\n",
    "    \n",
    "    print(f'Loss: {loss.numpy().mean()}')\n",
    "\n",
    "vocab_model.save_weights('../saved_model/base_model_saved/base_model_04/vocab_model_weights.h5')"
   ],
   "id": "d932a8d211006f33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Loss: 10.296875\n",
      "Epoch 2/10\n",
      "Loss: 10.296875\n",
      "Epoch 3/10\n",
      "Loss: 10.296875\n",
      "Epoch 4/10\n",
      "Loss: 10.296875\n",
      "Epoch 5/10\n",
      "Loss: 10.296875\n",
      "Epoch 6/10\n",
      "Loss: 10.296875\n",
      "Epoch 7/10\n",
      "Loss: 10.296875\n",
      "Epoch 8/10\n",
      "Loss: 10.296875\n",
      "Epoch 9/10\n",
      "Loss: 10.296875\n",
      "Epoch 10/10\n",
      "Loss: 10.296875\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Arsitektur Self-Attention Model",
   "id": "49ed15e2ed66119f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:44:04.794822Z",
     "start_time": "2024-08-06T19:44:04.631952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Buat optimizer baru\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Bungkus optimizer dengan LossScaleOptimizer hanya jika belum dibungkus\n",
    "if not getattr(optimizer, '_is_wrapped_by_loss_scale_optimizer', False):\n",
    "    optimizer = mixed_precision.LossScaleOptimizer(optimizer, loss_scale='dynamic')\n",
    "\n",
    "# Arsitektur Model Self-Attention\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"embedding_layer\")\n",
    "embedding_layer.build((None,))\n",
    "embedding_layer.set_weights(vocab_model.get_layer('embedding_layer').get_weights())\n",
    "\n",
    "def transformer_layer(x, d_model, num_heads, dff, rate=0.1):\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "    \n",
    "    ffn_output = tf.keras.Sequential([\n",
    "        Dense(dff, activation='relu'), \n",
    "        Dense(d_model)])(out1)\n",
    "    \n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "    \n",
    "    return out2\n",
    "\n",
    "num_transformer_layers = 1  # Mengurangi jumlah layer Transformer\n",
    "num_heads = 2  # Mengurangi jumlah heads\n",
    "dff = 128  # Mengurangi dimensi feed-forward\n",
    "\n",
    "inputs = Input(shape=(None,), name=\"inputs\")\n",
    "embedding = embedding_layer(inputs)\n",
    "\n",
    "transformer_output = embedding\n",
    "for _ in range(num_transformer_layers):\n",
    "    transformer_output = transformer_layer(transformer_output, embedding_dim, num_heads, dff)\n",
    "\n",
    "outputs = Dense(vocab_size, activation='softmax', name=\"output_layer\")(transformer_output)\n",
    "\n",
    "context_model = Model(inputs=inputs, outputs=outputs, name=\"context_model\")\n",
    "context_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "context_model.summary()"
   ],
   "id": "40590b97da1462cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. For example:\n",
      "  opt = tf.keras.mixed_precision.LossScaleOptimizer(opt)\n",
      "Model: \"context_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inputs (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_layer (Embedding)    (None, None, 64)     1897152     ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, None, 64)    33216       ['embedding_layer[0][0]',        \n",
      " eadAttention)                                                    'embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, None, 64)     0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, None, 64)    0           ['embedding_layer[0][0]',        \n",
      " mbda)                                                            'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, None, 64)    128         ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, None, 64)     16576       ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, None, 64)     0           ['sequential_1[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, None, 64)    0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, None, 64)    128         ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, None, 29643)  1926795     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,873,995\n",
      "Trainable params: 3,873,995\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset 2 Prep",
   "id": "d17dc84818d0dffb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:44:27.624655Z",
     "start_time": "2024-08-06T19:44:27.603425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Persiapan Data untuk Dataset 2\n",
    "dataset2_sequences = tokenizer.texts_to_sequences(dataset2_sampling)\n",
    "dataset2_padded = pad_sequences(dataset2_sequences, padding='post')\n",
    "\n",
    "dataset2_inputs = dataset2_padded[:, :-1]\n",
    "dataset2_targets = dataset2_padded[:, 1:]\n",
    "\n",
    "train_dataset2 = tf.data.Dataset.from_tensor_slices((dataset2_inputs, dataset2_targets))\n",
    "train_dataset2 = train_dataset2.shuffle(buffer_size=1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ],
   "id": "acc056153e7e44f4",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Model Self-Attention\n",
   "id": "b987c67d35a30a36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:44:29.067076Z",
     "start_time": "2024-08-06T19:44:28.416908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pelatihan Model Self-Attention dengan Gradient Accumulation\n",
    "@tf.function\n",
    "def train_step_attention(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = context_model(inputs, training=True)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(targets, predictions)\n",
    "    scaled_loss = loss / accumulation_steps\n",
    "    gradients = tape.gradient(scaled_loss, context_model.trainable_variables)\n",
    "    return gradients, loss\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(f'Epoch {epoch + 1}/{10}')\n",
    "    accum_gradients = [tf.zeros_like(var, dtype=tf.float32) for var in context_model.trainable_variables]\n",
    "    for step, (inputs, targets) in enumerate(train_dataset2):\n",
    "        gradients, loss = train_step_attention(inputs, targets)\n",
    "        accum_gradients = [\n",
    "            accum_grad + (grad if grad is not None else tf.zeros_like(accum_grad))\n",
    "            for accum_grad, grad in zip(accum_gradients, gradients)\n",
    "        ]\n",
    "        \n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            optimizer.apply_gradients(zip(accum_gradients, context_model.trainable_variables))\n",
    "            accum_gradients = [tf.zeros_like(var, dtype=tf.float32) for var in context_model.trainable_variables]\n",
    "    \n",
    "    if step % accumulation_steps != 0:\n",
    "        optimizer.apply_gradients(zip(accum_gradients, context_model.trainable_variables))\n",
    "    \n",
    "    print(f'Loss: {loss.numpy().mean()}')"
   ],
   "id": "2be290073ee9bcbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Loss: 10.2890625\n",
      "Epoch 2/10\n",
      "Loss: 10.2890625\n",
      "Epoch 3/10\n",
      "Loss: 10.2890625\n",
      "Epoch 4/10\n",
      "Loss: 10.2890625\n",
      "Epoch 5/10\n",
      "Loss: 10.2890625\n",
      "Epoch 6/10\n",
      "Loss: 10.2890625\n",
      "Epoch 7/10\n",
      "Loss: 10.2890625\n",
      "Epoch 8/10\n",
      "Loss: 10.2890625\n",
      "Epoch 9/10\n",
      "Loss: 10.2890625\n",
      "Epoch 10/10\n",
      "Loss: 10.2890625\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "914e512cf6242b6d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
