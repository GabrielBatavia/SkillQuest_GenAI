{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries",
   "id": "82ab7d7fe9448202"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T16:04:19.831741Z",
     "start_time": "2024-08-04T16:04:19.817588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Reshape, GlobalAveragePooling1D, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n"
   ],
   "id": "ff7962e100cfb7e3",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Embedding Model",
   "id": "a435b702fc5e7a2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T16:04:22.913565Z",
     "start_time": "2024-08-04T16:04:22.685054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the pre-trained embedding model\n",
    "embedding_model = tf.keras.models.load_model('../saved_model/base_model_saved/base_model_02/flexible_embedding_model.h5')\n",
    "\n",
    "\n",
    "# Display the model summary\n",
    "embedding_model.summary()"
   ],
   "id": "bc8b340adaeb5482",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 50)          1465950   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 50)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                3264      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 29319)             1905735   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,374,949\n",
      "Trainable params: 3,374,949\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Read and Clean Text Files",
   "id": "310a38918f860bae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:57:48.726961Z",
     "start_time": "2024-08-04T15:57:48.620369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to read and clean text files from a folder\n",
    "def read_text_files(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read().strip()\n",
    "                if content:  # Check if the file is not empty\n",
    "                    cleaned_content = clean_text(content)\n",
    "                    texts.append(cleaned_content)\n",
    "    return texts\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove unwanted characters\n",
    "    unwanted_chars = ['*', '#', '_', ')', '(', '!', '?', '.', ',', '-']\n",
    "    for char in unwanted_chars:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n",
    "\n",
    "# Read texts from the new dataset\n",
    "folder_path = '../Dataset/nlp_dataset'\n",
    "texts = read_text_files(folder_path)\n"
   ],
   "id": "f77d39daec9d0be8",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocess Text Data",
   "id": "b5f068aaf7b296ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T16:04:58.925401Z",
     "start_time": "2024-08-04T16:04:58.718265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to sequence of numbers\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=100)  # Adjust maxlen as needed\n",
    "    return padded_sequence\n"
   ],
   "id": "e73bedb72f91c21f",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split Dataset",
   "id": "764158a3fc44ad4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:58:07.493039Z",
     "start_time": "2024-08-04T15:58:07.484198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split dataset into train, validation, and test sets\n",
    "train_texts = texts[:int(0.8 * len(texts))]\n",
    "val_texts = texts[int(0.8 * len(texts)):int(0.9 * len(texts))]\n",
    "test_texts = texts[int(0.9 * len(texts)):]\n",
    "\n",
    "# Example labels (assuming all labels are 1, adjust as per your dataset)\n",
    "train_labels = [1] * len(train_texts)\n",
    "val_labels = [1] * len(val_texts)\n"
   ],
   "id": "a94d02de0577cbf5",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Attention Layer",
   "id": "fc2cacf28a398014"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T16:08:58.669332Z",
     "start_time": "2024-08-04T16:08:58.650716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Attention Layer\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], input_shape[-1]), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[-1],), initializer='zeros', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=[2, 0]) + self.b)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector\n"
   ],
   "id": "a9b20780ec312b2a",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define New Model",
   "id": "54a5960e3b4141e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T16:08:59.787146Z",
     "start_time": "2024-08-04T16:08:59.460846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define new flexible model with LSTM and Attention using Functional API\n",
    "input_layer = Input(shape=(100,), name='input_layer')  # Adjust input shape to match the padded sequence length\n",
    "\n",
    "# Use the embedding layer from the loaded model\n",
    "embedding_output = embedding_model.layers[0](input_layer)\n",
    "\n",
    "# Add LSTM layers\n",
    "lstm_output = LSTM(128, return_sequences=True, name='lstm_1')(embedding_output)\n",
    "lstm_output = LSTM(64, return_sequences=True, name='lstm_2')(lstm_output)\n",
    "\n",
    "# Add Attention layer\n",
    "attention_output = AttentionLayer(name='attention_layer')(lstm_output)\n",
    "\n",
    "# Add Global Average Pooling for more flexibility\n",
    "global_avg_pool = GlobalAveragePooling1D(name='global_avg_pooling')(attention_output)\n",
    "\n",
    "# Add Dense layer for classification task\n",
    "dense_output = Dense(64, activation='relu', name='dense_1')(global_avg_pool)\n",
    "output_layer = Dense(1, activation='sigmoid', name='output_layer')(dense_output)  # Example for binary classification\n",
    "\n",
    "# Create model\n",
    "flexible_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile model\n",
    "flexible_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "flexible_model.summary()\n"
   ],
   "id": "9c3110512b4367e2",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"global_avg_pooling\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 64)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[32], line 15\u001B[0m\n\u001B[0;32m     12\u001B[0m attention_output \u001B[38;5;241m=\u001B[39m AttentionLayer(name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_layer\u001B[39m\u001B[38;5;124m'\u001B[39m)(lstm_output)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Add Global Average Pooling for more flexibility\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m global_avg_pool \u001B[38;5;241m=\u001B[39m \u001B[43mGlobalAveragePooling1D\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mglobal_avg_pooling\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattention_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# Add Dense layer for classification task\u001B[39;00m\n\u001B[0;32m     18\u001B[0m dense_output \u001B[38;5;241m=\u001B[39m Dense(\u001B[38;5;241m64\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdense_1\u001B[39m\u001B[38;5;124m'\u001B[39m)(global_avg_pool)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\input_spec.py:214\u001B[0m, in \u001B[0;36massert_input_compatibility\u001B[1;34m(input_spec, inputs, layer_name)\u001B[0m\n\u001B[0;32m    212\u001B[0m   ndim \u001B[38;5;241m=\u001B[39m shape\u001B[38;5;241m.\u001B[39mrank\n\u001B[0;32m    213\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m ndim \u001B[38;5;241m!=\u001B[39m spec\u001B[38;5;241m.\u001B[39mndim:\n\u001B[1;32m--> 214\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInput \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of layer \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    215\u001B[0m                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mis incompatible with the layer: \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    216\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexpected ndim=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mspec\u001B[38;5;241m.\u001B[39mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, found ndim=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    217\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFull shape received: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtuple\u001B[39m(shape)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    218\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spec\u001B[38;5;241m.\u001B[39mmax_ndim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    219\u001B[0m   ndim \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;241m.\u001B[39mrank\n",
      "\u001B[1;31mValueError\u001B[0m: Input 0 of layer \"global_avg_pooling\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 64)"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convert Text to Embedding",
   "id": "a892481edcacea24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T16:03:32.034670Z",
     "start_time": "2024-08-04T16:03:11.935601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to convert text to embedding\n",
    "def text_to_embedding(text, model):\n",
    "    processed_text = preprocess_text(text)\n",
    "    embedding = model.predict(processed_text)\n",
    "    return embedding\n",
    "\n",
    "# Convert dataset to embeddings\n",
    "train_embeddings = [text_to_embedding(text, embedding_model) for text in train_texts]\n",
    "val_embeddings = [text_to_embedding(text, embedding_model) for text in val_texts]\n",
    "\n",
    "# Convert embeddings to numpy array and add sequence length dimension\n",
    "train_embeddings = np.expand_dims(np.array(train_embeddings), axis=1)\n",
    "val_embeddings = np.expand_dims(np.array(val_embeddings), axis=1)\n",
    "\n",
    "# Convert labels to numpy array\n",
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)\n"
   ],
   "id": "58b7865c2fee8d80",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train New Model",
   "id": "a7660dad79afdc7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T16:03:51.891018Z",
     "start_time": "2024-08-04T16:03:51.732980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the new model\n",
    "new_model.fit(train_embeddings, train_labels, epochs=10, validation_data=(val_embeddings, val_labels))\n"
   ],
   "id": "1132ccd295e0a6a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 100), found shape=(None, 1, 1, 29319)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Train the new model\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mnew_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_embeddings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mval_embeddings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[0;32m   1146\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1147\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mag_error_metadata\u001B[38;5;241m.\u001B[39mto_exception(e)\n\u001B[0;32m   1148\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1149\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 100), found shape=(None, 1, 1, 29319)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cell 10: Evaluate and Predict",
   "id": "144b170c19ea4efe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4852262208af6b42"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
