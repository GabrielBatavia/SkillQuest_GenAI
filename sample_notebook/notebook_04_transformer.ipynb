{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Arsitektur Model Transformer",
   "id": "41975b544ad4b481"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "                        +-----------------------+\n",
    "                        |                       |\n",
    "                        |      Input Layer      |\n",
    "                        | (text_input: shape)   |\n",
    "                        |                       |\n",
    "                        +-----------+-----------+\n",
    "                                    |\n",
    "                                    v\n",
    "                        +-----------+-----------+\n",
    "                        |                       |\n",
    "                        |   Embedding Layer     |\n",
    "                        |                       |\n",
    "                        +-----------+-----------+\n",
    "                                    |\n",
    "                                    v\n",
    "                        +-----------+-----------+\n",
    "                        |                       |\n",
    "                        | Multi-Head Attention  |\n",
    "                        |                       |\n",
    "                        +-----------+-----------+\n",
    "                                    |\n",
    "                                    v\n",
    "                        +-----------+-----------+\n",
    "                        |                       |\n",
    "                        |  Layer Normalization  |\n",
    "                        |                       |\n",
    "                        +-----------+-----------+\n",
    "                                    |\n",
    "                                    v\n",
    "                        +-----------+-----------+\n",
    "                        |                       |\n",
    "                        |   Feed-Forward Layer  |\n",
    "                        |                       |\n",
    "                        +-----------+-----------+\n",
    "                                    |\n",
    "                                    v\n",
    "                        +-----------+-----------+\n",
    "                        |                       |\n",
    "                        |  Layer Normalization  |\n",
    "                        |                       |\n",
    "                        +-----------+-----------+\n",
    "                                    |\n",
    "                                    v\n",
    "                        +-----------+-----------+\n",
    "                        |                       |\n",
    "                        |       Dropout         |\n",
    "                        |                       |\n",
    "                        +-----------+-----------+\n",
    "                                    |\n",
    "                                    v\n",
    "                        +-----------+-----------+\n",
    "                        |                       |\n",
    "                        |     Output Layer      |\n",
    "                        |                       |\n",
    "                        +-----------------------+\n"
   ],
   "id": "b6d9c5c2348e4b02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Penjelasan Mengapa Memakai Model Ini\n",
    "\n",
    "Kemampuan Self-Attention:\n",
    "- Transformer menggunakan mekanisme self-attention yang memungkinkan model untuk memperhatikan semua kata dalam urutan input secara bersamaan. Ini membuatnya sangat efektif dalam menangkap dependensi jarak jauh dan hubungan kontekstual dalam teks.\n",
    "\n",
    "Pelatihan Paralel:\n",
    "- Tidak seperti model RNN/LSTM yang memproses data secara berurutan, Transformer dapat memproses seluruh urutan input secara paralel. Ini membuat pelatihan lebih cepat dan lebih efisien, terutama dengan dataset besar.\n",
    "\n",
    "Fleksibilitas:\n",
    "- Transformer dapat dengan mudah diskalakan dan dimodifikasi untuk berbagai tugas NLP, termasuk terjemahan mesin, pemahaman bacaan, dan generasi teks. Model ini dapat digunakan untuk tugas-tugas pemahaman konteks yang kompleks.\n",
    "\n",
    "Kinerja Terbaik dalam Berbagai Tugas NLP:\n",
    "- Transformer telah menunjukkan kinerja yang unggul dalam berbagai benchmark dan tugas NLP. Arsitektur ini telah menjadi standar emas untuk banyak aplikasi NLP.\n",
    "\n",
    "#### Keunggulan Model Transformer\n",
    "\n",
    "Pemahaman Konteks yang Baik:\n",
    "\n",
    "- Kemampuan self-attention memungkinkan model untuk memahami konteks setiap kata dalam kalimat secara lebih baik dibandingkan dengan model sekuensial tradisional.\n",
    "\n",
    "Skalabilitas:\n",
    "- Model Transformer dapat dengan mudah ditingkatkan dengan menambahkan lebih banyak layer dan unit tanpa mempengaruhi kinerja pelatihan secara signifikan.\n",
    "\n",
    "Generalisasi yang Lebih Baik:\n",
    "- Dengan menangkap dependensi jarak jauh dalam teks, Transformer cenderung memiliki kemampuan generalisasi yang lebih baik, membuatnya efektif dalam menangani berbagai jenis teks."
   ],
   "id": "1685e41465f8901e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Diagram Arsitektur yang Lebih Rinci",
   "id": "480327b129008769"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "                               +-----------------------+\n",
    "                               |      Input Layer      |\n",
    "                               | (text_input: shape)   |\n",
    "                               +-----------+-----------+\n",
    "                                           |\n",
    "                                           v\n",
    "                               +-----------+-----------+\n",
    "                               |   Embedding Layer     |\n",
    "                               |                       |\n",
    "                               +-----------+-----------+\n",
    "                                           |\n",
    "                                           v\n",
    "                +--------------------------+--------------------------+\n",
    "                |                         |                          |\n",
    "                |  Multi-Head Attention   |  Multi-Head Attention    |\n",
    "                | (Self-Attention)        | (Cross-Attention)        |\n",
    "                +-----------+-------------+-------------+------------+\n",
    "                            |                           |\n",
    "                            v                           v\n",
    "                +-----------+---------------------------+------------+\n",
    "                |           |         Layer Normalization            |\n",
    "                +-----------+---------------------------+------------+\n",
    "                            |                           |\n",
    "                            v                           v\n",
    "                +-----------+-------------+-------------+------------+\n",
    "                |  Feed-Forward Layer     |  Feed-Forward Layer      |\n",
    "                +-----------+-------------+-------------+------------+\n",
    "                            |                           |\n",
    "                            v                           v\n",
    "                +-----------+---------------------------+------------+\n",
    "                |           |         Layer Normalization            |\n",
    "                +-----------+---------------------------+------------+\n",
    "                            |                           |\n",
    "                            v                           v\n",
    "                +-----------+---------------------------+------------+\n",
    "                |                       Dropout                       |\n",
    "                +-----------+---------------------------+------------+\n",
    "                                           |\n",
    "                                           v\n",
    "                               +-----------+-----------+\n",
    "                               |       Output Layer    |\n",
    "                               +-----------------------+\n"
   ],
   "id": "5ad3e690c0b29216"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Langkah 1: Mengimpor dan Memverifikasi GPU",
   "id": "a5bd9f5023facadb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Verifikasi GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n"
   ],
   "id": "4eaad4d3aebc7edb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Langkah 2: Fungsi untuk Memuat dan Memproses Data",
   "id": "8f87d32fd76f032e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Fungsi untuk memuat file JSON dari folder\n",
    "def load_json_files(data_folder):\n",
    "    data = []\n",
    "    for filename in os.listdir(data_folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(data_folder, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    content = json.load(file)\n",
    "                    if content:  # Check if file is not empty\n",
    "                        data.append(content)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping file {filename} due to JSON decoding error.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping file {filename} due to unexpected error: {e}\")\n",
    "    return data\n",
    "\n",
    "# Fungsi untuk memproses data menjadi teks dan jawaban\n",
    "def preprocess_data(data):\n",
    "    texts = []\n",
    "    answers = []\n",
    "    for article in data:\n",
    "        if 'konten' in article:\n",
    "            texts.append(article['konten'])\n",
    "            # Asumsikan bahwa jawaban adalah konten itu sendiri (untuk latihan ini)\n",
    "            answers.append(article['konten'])\n",
    "    return texts, answers\n"
   ],
   "id": "c606fe5f9ff61eec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Langkah 3: Tokenisasi dan Vektorisasi Data",
   "id": "9b8751c8c3e475ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fungsi untuk tokenisasi dan vektorisasi teks dan jawaban\n",
    "def tokenize_and_vectorize(texts, answers, max_vocab_size=10000, max_seq_length=100):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_vocab_size, oov_token='<UNK>')\n",
    "    tokenizer.fit_on_texts(texts + answers)\n",
    "    \n",
    "    text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    answer_sequences = tokenizer.texts_to_sequences(answers)\n",
    "    \n",
    "    # Verifikasi distribusi token <UNK>\n",
    "    unk_texts = sum([seq.count(1) for seq in text_sequences]) / len(text_sequences)\n",
    "    unk_answers = sum([seq.count(1) for seq in answer_sequences]) / len(answer_sequences)\n",
    "    print(f\"Proportion of <UNK> tokens in texts: {unk_texts:.2f}\")\n",
    "    print(f\"Proportion of <UNK> tokens in answers: {unk_answers:.2f}\")\n",
    "\n",
    "    padded_texts = tf.keras.preprocessing.sequence.pad_sequences(text_sequences, maxlen=max_seq_length, padding='post')\n",
    "    padded_answers = tf.keras.preprocessing.sequence.pad_sequences(answer_sequences, maxlen=max_seq_length, padding='post')\n",
    "    \n",
    "    return padded_texts, padded_answers, tokenizer\n"
   ],
   "id": "471142d413d649d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Langkah 4: Membuat Pipeline Data",
   "id": "262dc57788ccaf71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fungsi untuk membuat pipeline data\n",
    "def create_data_pipeline(padded_texts, padded_answers, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((padded_texts, padded_answers), padded_answers))\n",
    "    dataset = dataset.shuffle(buffer_size=len(padded_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n"
   ],
   "id": "d0585d4c994d1479",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Langkah 5: Membuat Model Transformer",
   "id": "30c1c87ec22bf3ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Dense, LayerNormalization, MultiHeadAttention, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def transformer_encoder(vocab_size, num_heads, ff_dim, num_layers, embedding_dim, max_seq_length):\n",
    "    text_input = Input(shape=(max_seq_length,), name='text_input')\n",
    "    answer_input = Input(shape=(max_seq_length,), name='answer_input')\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim)\n",
    "    text_embedding = embedding_layer(text_input)\n",
    "    answer_embedding = embedding_layer(answer_input)\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        # Encoder\n",
    "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(text_embedding, text_embedding)\n",
    "        attn_output = Dropout(0.1)(attn_output)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(text_embedding + attn_output)\n",
    "        \n",
    "        ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
    "        ffn_output = Dense(embedding_dim)(ffn_output)\n",
    "        ffn_output = Dropout(0.1)(ffn_output)\n",
    "        text_embedding = LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "        \n",
    "        # Decoder\n",
    "        dec_attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(answer_embedding, text_embedding)\n",
    "        dec_attn_output = Dropout(0.1)(dec_attn_output)\n",
    "        out2 = LayerNormalization(epsilon=1e-6)(answer_embedding + dec_attn_output)\n",
    "        \n",
    "        ffn_output = Dense(ff_dim, activation='relu')(out2)\n",
    "        ffn_output = Dense(embedding_dim)(ffn_output)\n",
    "        ffn_output = Dropout(0.1)(ffn_output)\n",
    "        answer_embedding = LayerNormalization(epsilon=1e-6)(out2 + ffn_output)\n",
    "    \n",
    "    output = Dense(vocab_size, activation='softmax')(answer_embedding)\n",
    "    \n",
    "    model = Model(inputs=[text_input, answer_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Membuat model dengan parameter yang ditingkatkan\n",
    "vocab_size = 10000\n",
    "num_heads = 4  # Kurangi jumlah heads\n",
    "ff_dim = 512  # Kurangi dimensi feed-forward\n",
    "num_layers = 4  # Kurangi jumlah lapisan\n",
    "embedding_dim = 256  # Kurangi dimensi embedding\n",
    "max_seq_length = 100\n",
    "\n",
    "transformer_model = transformer_encoder(vocab_size, num_heads, ff_dim, num_layers, embedding_dim, max_seq_length)\n"
   ],
   "id": "aa04f647fc160c9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Langkah 6: Melatih Model",
   "id": "8d3e49a03f046315"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load and preprocess data\n",
    "folder_path = './Dataset/generative_dataset'\n",
    "data = load_json_files(folder_path)\n",
    "texts, answers = preprocess_data(data)\n",
    "padded_texts, padded_answers, tokenizer = tokenize_and_vectorize(texts, answers)"
   ],
   "id": "83e1cd113d0838",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Create data pipeline\n",
    "batch_size = 32\n",
    "data_pipeline = create_data_pipeline(padded_texts, padded_answers, batch_size)\n",
    "\n",
    "# Compile the model\n",
    "transformer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Melatih model dengan early stopping dan model checkpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('transformer_model.h5', save_best_only=True)\n",
    "\n",
    "# Melatih model\n",
    "epochs = 50\n",
    "history = transformer_model.fit(data_pipeline, epochs=epochs, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "\n",
    "# Menyimpan model setelah pelatihan\n",
    "transformer_model.save('./saved_model/saved_notebook_04/transformer_model.h5')\n",
    "\n",
    "# Menyimpan tokenizer\n",
    "import pickle\n",
    "\n",
    "with open('./saved_model/saved_notebook_04/tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n"
   ],
   "id": "cb9c1747af23333d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Langkah 7: Visualisasi Model dan Hasil Pelatihan",
   "id": "b8a631db0c132f26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting model architecture\n",
    "tf.keras.utils.plot_model(transformer_model, to_file='transformer_model_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Visualisasi loss dan akurasi\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Menampilkan hasil pelatihan\n",
    "plot_training_history(history)\n"
   ],
   "id": "d6b4183825b0b3b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Langkah 8: Model Summary",
   "id": "dea75799b86cbdfb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Menampilkan arsitektur model\n",
    "transformer_model.summary()\n"
   ],
   "id": "40018b4c6cd4e08f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7923fcb4c3168c8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Langkah 9: Memuat Model dan Tokenizer, dan Fungsi untuk Menghasilkan Output",
   "id": "3f79c3264cd80a7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Memuat model yang telah disimpan\n",
    "transformer_model = tf.keras.models.load_model('./saved_model/saved_notebook_04/transformer_model.h5')\n",
    "\n",
    "# Memuat tokenizer\n",
    "with open('./saved_model/saved_notebook_04/tokenizer.pkl', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)\n",
    "\n",
    "# Fungsi untuk menghasilkan teks dari input teks\n",
    "def generate_text(input_text):\n",
    "    input_seq = tokenizer.texts_to_sequences([input_text])\n",
    "    padded_input = tf.keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=100, padding='post')\n",
    "\n",
    "    # Inisialisasi jawaban dengan urutan kosong\n",
    "    decoded_sentence = []\n",
    "\n",
    "    for _ in range(100):\n",
    "        padded_answer = tf.keras.preprocessing.sequence.pad_sequences([decoded_sentence], maxlen=100, padding='post')\n",
    "\n",
    "        # Menggunakan model untuk memprediksi output\n",
    "        predicted_seq = transformer_model.predict([padded_input, padded_answer])\n",
    "\n",
    "        # Mendapatkan token yang diprediksi\n",
    "        predicted_token = predicted_seq.argmax(axis=-1)[0, len(decoded_sentence)]\n",
    "        decoded_sentence.append(predicted_token)\n",
    "\n",
    "    # Mengkonversi token menjadi teks\n",
    "    generated_text = tokenizer.sequences_to_texts([decoded_sentence])[0]\n",
    "    return generated_text\n",
    "\n",
    "# Fungsi untuk menerima input dan menghasilkan output\n",
    "def interactive_text_generation():\n",
    "    while True:\n",
    "        user_input = input(\"Masukkan pertanyaan atau 'exit' untuk keluar: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        generated_text = generate_text(user_input)\n",
    "        print(f\"Teks yang dihasilkan:\\n{generated_text}\")\n",
    "\n",
    "# Memulai interaksi\n",
    "interactive_text_generation()"
   ],
   "id": "7a7aaac6a9ad35ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1ecb1fd5d1ee3dee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8c77761fd149f09e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c63f9a4e258e2bfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cell 1: Mengimpor dan Memverifikasi GPU",
   "id": "a3e26fd86ee0b81f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Verifikasi GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n"
   ],
   "id": "e19a9d6865a3835a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cell 2: Fungsi untuk Memuat dan Memproses Data",
   "id": "45b4c363a0c57a90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Fungsi untuk memuat file JSON dari folder\n",
    "def load_json_files(data_folder):\n",
    "    data = []\n",
    "    for filename in os.listdir(data_folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(data_folder, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    content = json.load(file)\n",
    "                    if content:  # Check if file is not empty\n",
    "                        data.append(content)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping file {filename} due to JSON decoding error.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping file {filename} due to unexpected error: {e}\")\n",
    "    return data\n",
    "\n",
    "# Fungsi untuk memproses data menjadi teks dan jawaban\n",
    "def preprocess_data(data):\n",
    "    texts = []\n",
    "    answers = []\n",
    "    for article in data:\n",
    "        if 'konten' in article:\n",
    "            texts.append(article['konten'])\n",
    "            # Asumsikan bahwa jawaban adalah konten itu sendiri (untuk latihan ini)\n",
    "            answers.append(article['konten'])\n",
    "    return texts, answers\n"
   ],
   "id": "2488263b49c0d4fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cell 3: Tokenisasi dan Vektorisasi Data",
   "id": "89c42fc44bf91a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fungsi untuk tokenisasi dan vektorisasi teks dan jawaban\n",
    "def tokenize_and_vectorize(texts, answers, max_vocab_size=10000, max_seq_length=100):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_vocab_size, oov_token='<UNK>')\n",
    "    tokenizer.fit_on_texts(texts + answers)\n",
    "    \n",
    "    text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    answer_sequences = tokenizer.texts_to_sequences(answers)\n",
    "    \n",
    "    # Verifikasi distribusi token <UNK>\n",
    "    unk_texts = sum([seq.count(1) for seq in text_sequences]) / len(text_sequences)\n",
    "    unk_answers = sum([seq.count(1) for seq in answer_sequences]) / len(answer_sequences)\n",
    "    print(f\"Proportion of <UNK> tokens in texts: {unk_texts:.2f}\")\n",
    "    print(f\"Proportion of <UNK> tokens in answers: {unk_answers:.2f}\")\n",
    "\n",
    "    padded_texts = tf.keras.preprocessing.sequence.pad_sequences(text_sequences, maxlen=max_seq_length, padding='post')\n",
    "    padded_answers = tf.keras.preprocessing.sequence.pad_sequences(answer_sequences, maxlen=max_seq_length, padding='post')\n",
    "    \n",
    "    return padded_texts, padded_answers, tokenizer\n"
   ],
   "id": "196b00122a38055e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cell 4: Membuat Pipeline Data",
   "id": "88eb13055a4c3936"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fungsi untuk membuat pipeline data\n",
    "def create_data_pipeline(padded_texts, padded_answers, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((padded_texts, padded_answers), padded_answers))\n",
    "    dataset = dataset.shuffle(buffer_size=len(padded_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n"
   ],
   "id": "21638b4998a7490b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cell 5: Membuat Model Transformer",
   "id": "1e341b5b1b3c2694"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Dense, LayerNormalization, MultiHeadAttention, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def transformer_encoder(vocab_size, num_heads, ff_dim, num_layers, embedding_dim, max_seq_length):\n",
    "    text_input = Input(shape=(max_seq_length,), name='text_input')\n",
    "    answer_input = Input(shape=(max_seq_length,), name='answer_input')\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim)\n",
    "    text_embedding = embedding_layer(text_input)\n",
    "    answer_embedding = embedding_layer(answer_input)\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        # Encoder\n",
    "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(text_embedding, text_embedding)\n",
    "        attn_output = Dropout(0.2)(attn_output)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(text_embedding + attn_output)\n",
    "        \n",
    "        ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
    "        ffn_output = Dense(embedding_dim)(ffn_output)\n",
    "        ffn_output = Dropout(0.2)(ffn_output)\n",
    "        text_embedding = LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "        \n",
    "        # Decoder\n",
    "        dec_attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(answer_embedding, text_embedding)\n",
    "        dec_attn_output = Dropout(0.2)(dec_attn_output)\n",
    "        out2 = LayerNormalization(epsilon=1e-6)(answer_embedding + dec_attn_output)\n",
    "        \n",
    "        ffn_output = Dense(ff_dim, activation='relu')(out2)\n",
    "        ffn_output = Dense(embedding_dim)(ffn_output)\n",
    "        ffn_output = Dropout(0.2)(ffn_output)\n",
    "        answer_embedding = LayerNormalization(epsilon=1e-6)(out2 + ffn_output)\n",
    "    \n",
    "    output = Dense(vocab_size, activation='softmax')(answer_embedding)\n",
    "    \n",
    "    model = Model(inputs=[text_input, answer_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Membuat model dengan parameter yang ditingkatkan\n",
    "vocab_size = 10000\n",
    "num_heads = 4  # Kurangi jumlah heads\n",
    "ff_dim = 512  # Kurangi dimensi feed-forward\n",
    "num_layers = 4  # Kurangi jumlah lapisan\n",
    "embedding_dim = 256  # Kurangi dimensi embedding\n",
    "max_seq_length = 100\n",
    "\n",
    "transformer_model = transformer_encoder(vocab_size, num_heads, ff_dim, num_layers, embedding_dim, max_seq_length)\n"
   ],
   "id": "16990dc2e7b9c699",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cell 6: Membagi Data Menjadi Pelatihan dan Validasi",
   "id": "555fae1decfbbb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess data\n",
    "folder_path = './Dataset/generative_dataset'\n",
    "data = load_json_files(folder_path)\n",
    "texts, answers = preprocess_data(data)\n",
    "padded_texts, padded_answers, tokenizer = tokenize_and_vectorize(texts, answers)\n",
    "\n",
    "# Membagi data menjadi pelatihan dan validasi\n",
    "padded_texts_train, padded_texts_val, padded_answers_train, padded_answers_val = train_test_split(\n",
    "    padded_texts, padded_answers, test_size=0.2, random_state=42)\n"
   ],
   "id": "f7447a67ef9ad281",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cell 7: Membuat Pipeline Data untuk Pelatihan dan Validasi",
   "id": "a6ed1afe46854867"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Membuat pipeline data untuk pelatihan dan validasi\n",
    "batch_size = 16  # Kurangi ukuran batch untuk mengurangi penggunaan memori\n",
    "train_data_pipeline = create_data_pipeline(padded_texts_train, padded_answers_train, batch_size)\n",
    "val_data_pipeline = create_data_pipeline(padded_texts_val, padded_answers_val, batch_size)\n"
   ],
   "id": "ce1ec87e82ddcac5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cell 8: Melatih Model dengan Data Validasi",
   "id": "703403320bc41a8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compile the model\n",
    "transformer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Melatih model dengan early stopping dan model checkpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('./saved_model/saved_notebook_04/transformer_model.h5', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Melatih model\n",
    "epochs = 50\n",
    "history = transformer_model.fit(\n",
    "    train_data_pipeline, \n",
    "    epochs=epochs, \n",
    "    validation_data=val_data_pipeline, \n",
    "    callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Menyimpan tokenizer\n",
    "import pickle\n",
    "\n",
    "with open('./saved_model/saved_notebook_04/tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)"
   ],
   "id": "acd03bbc14f325f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cell 9: Evaluasi Model",
   "id": "8e138ec72787b300"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualisasi loss dan akurasi\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Menampilkan hasil pelatihan\n",
    "plot_training_history(history)\n"
   ],
   "id": "2f1dcc9376c4d68c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Menampilkan arsitektur model\n",
    "transformer_model.summary()"
   ],
   "id": "11a6f2018f4a51f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cell 10: Memuat Model dan Tokenizer, dan Fungsi untuk Menghasilkan Output",
   "id": "80c2fae8b7433ae8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Memuat model yang telah disimpan\n",
    "transformer_model = tf.keras.models.load_model('./saved_model/saved_notebook_04/transformer_model.h5')\n",
    "\n",
    "# Memuat tokenizer\n",
    "with open('./saved_model/saved_notebook_04/tokenizer.pkl', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)\n",
    "\n",
    "# Fungsi untuk menghasilkan teks dari input teks\n",
    "def generate_text(input_text):\n",
    "    input_seq = tokenizer.texts_to_sequences([input_text])\n",
    "    padded_input = tf.keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=100, padding='post')\n",
    "\n",
    "    # Inisialisasi jawaban dengan urutan kosong\n",
    "    decoded_sentence = []\n",
    "\n",
    "    for _ in range(100):\n",
    "        padded_answer = tf.keras.preprocessing.sequence.pad_sequences([decoded_sentence], maxlen=100, padding='post')\n",
    "\n",
    "        # Menggunakan model untuk memprediksi output\n",
    "        predicted_seq = transformer_model.predict([padded_input, padded_answer])\n",
    "\n",
    "        # Mendapatkan token yang diprediksi\n",
    "        predicted_token = predicted_seq.argmax(axis=-1)[0, len(decoded_sentence)]\n",
    "        decoded_sentence.append(predicted_token)\n",
    "\n",
    "    # Mengkonversi token menjadi teks\n",
    "    generated_text = tokenizer.sequences_to_texts([decoded_sentence])[0]\n",
    "    return generated_text\n",
    "\n",
    "# Fungsi untuk menerima input dan menghasilkan output\n",
    "def interactive_text_generation():\n",
    "    while True:\n",
    "        user_input = input(\"Masukkan pertanyaan atau 'exit' untuk keluar: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        generated_text = generate_text(user_input)\n",
    "        print(f\"Teks yang dihasilkan:\\n{generated_text}\")\n",
    "\n",
    "# Memulai interaksi\n",
    "interactive_text_generation()\n"
   ],
   "id": "cec7592f4994966b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Rangkuman Model Transformer",
   "id": "32810bed16140a7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Model Transformer yang kita bangun menggunakan arsitektur yang terdiri dari beberapa komponen penting:\n",
    "\n",
    "Input Layers:\n",
    "\n",
    "- Dua input layers: text_input dan answer_input, masing-masing dengan bentuk (max_seq_length,).\n",
    "\n",
    "Embedding Layer:\n",
    "\n",
    "- Kedua input diteruskan melalui embedding layer yang sama dengan dimensi embedding embedding_dim.\n",
    "\n",
    "Transformer Layers:\n",
    "\n",
    "Lapisan-lapisan Transformer yang terdiri dari beberapa komponen utama:\n",
    "- Multi-Head Attention: Mechanisme self-attention dan cross-attention.\n",
    "- Layer Normalization: Normalisasi setelah penjumlahan residual.\n",
    "Feed-Forward Networks (FFN): Dua dense layers dengan aktivasi ReLU di antara keduanya.\n",
    "- Dropout: Mencegah overfitting dengan dropout layers setelah multi-head attention dan FFN.\n",
    "\n",
    "Output Layer:\n",
    "\n",
    "- Dense layer dengan aktivasi softmax untuk menghasilkan distribusi probabilitas atas vocabulary."
   ],
   "id": "29dcb782350ffeba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Saran Peningkatan Kedepannya\n",
   "id": "6e92fe162136c513"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Hyperparameter Tuning:\n",
    "\n",
    "- Eksperimen dengan berbagai kombinasi hyperparameter seperti jumlah heads, dimensi feed-forward, jumlah lapisan, dan dimensi embedding.\n",
    "- Gunakan alat seperti Optuna atau Keras Tuner untuk melakukan pencarian hyperparameter secara otomatis.\n",
    "\n",
    "Preprocessing Lebih Lanjut:\n",
    "- Tambahkan langkah preprocessing tambahan jika diperlukan, seperti stemming atau lemmatization, jika data memiliki banyak bentuk kata yang berbeda.\n",
    "\n",
    "Augmentasi Data:\n",
    "- Gunakan teknik augmentasi data seperti penggantian sinonim, penghapusan kata acak, atau penambahan kata acak untuk meningkatkan jumlah dan variasi data pelatihan.\n",
    "\n",
    "Model Ensembling:\n",
    "- Gunakan teknik ensembling dengan menggabungkan beberapa model Transformer untuk meningkatkan kinerja secara keseluruhan.\n",
    "\n",
    "Transfer Learning:\n",
    "- Pertimbangkan menggunakan model pralatih seperti BERT, GPT, atau T5 sebagai dasar dan fine-tuning dengan dataset spesifik Anda.\n",
    "\n",
    "Peningkatan Infrastruktur:\n",
    "- Pertimbangkan menggunakan GPU yang lebih kuat atau cluster GPU untuk mempercepat pelatihan model, terutama dengan dataset besar dan model yang lebih kompleks.\n",
    "\n",
    "Regularisasi Lebih Lanjut:\n",
    "- Eksperimen dengan teknik regularisasi tambahan seperti weight decay atau lebih banyak dropout untuk mencegah overfitting.\n",
    "\n",
    "Evaluasi dengan Metric Berbeda:\n",
    "- Selain accuracy dan loss, pertimbangkan menggunakan metrik lain seperti F1 score, BLEU score, atau ROUGE score untuk evaluasi kinerja model dalam tugas generasi teks."
   ],
   "id": "689646b96f8fa4f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a7631094f438267e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
