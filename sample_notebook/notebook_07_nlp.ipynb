{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Arsitektur Model",
   "id": "3d59f85e9de23d76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "                                +----------------+\n",
    "                                |   Input Text   |\n",
    "                                +--------+-------+\n",
    "                                         |\n",
    "                                         v\n",
    "                                +--------+--------+\n",
    "                                |  SentencePiece  |\n",
    "                                |   Tokenization  |\n",
    "                                +--------+--------+\n",
    "                                         |\n",
    "                                         v\n",
    "                                +--------+--------+\n",
    "                                |  Embedding      |\n",
    "                                |  Layer          |\n",
    "                                +--------+--------+\n",
    "                                         |\n",
    "                                         v\n",
    "                                +--------+--------+\n",
    "                                |  Positional     |\n",
    "                                |  Encoding       |\n",
    "                                +--------+--------+\n",
    "                                         |\n",
    "                                         v\n",
    "                       +-----------------+-----------------+\n",
    "                       |                                     |\n",
    "                       v                                     v\n",
    "              +--------+--------+                  +--------+--------+\n",
    "              | Transformer     |                  | Transformer     |\n",
    "              | Encoder Block   |                  | Decoder Block   |\n",
    "              +--------+--------+                  +--------+--------+\n",
    "                       |                                     |\n",
    "                       v                                     v\n",
    "              +--------+--------+                  +--------+--------+\n",
    "              | Transformer     |                  | Transformer     |\n",
    "              | Encoder Block   |                  | Decoder Block   |\n",
    "              +--------+--------+                  +--------+--------+\n",
    "                       |                                     |\n",
    "                       v                                     v\n",
    "              +--------+--------+                  +--------+--------+\n",
    "              | Transformer     |                  | Transformer     |\n",
    "              | Encoder Block   |                  | Decoder Block   |\n",
    "              +--------+--------+                  +--------+--------+\n",
    "                       |                                     |\n",
    "                       v                                     v\n",
    "              +--------+--------+                  +--------+--------+\n",
    "              | Flatten         |                  | Final Dense    |\n",
    "              | Layer           |                  | Layer          |\n",
    "              +--------+--------+                  +--------+--------+\n",
    "                       |                                     |\n",
    "                       v                                     v\n",
    "               +-------+--------+                   +--------+--------+\n",
    "               |      Output     |                   |    Output       |\n",
    "               +-----------------+                   +-----------------+\n"
   ],
   "id": "83dd13b04dc9e2db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Libraries and Define Helper Functions",
   "id": "aa6105bb0f37a7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-03T14:20:22.057811Z",
     "start_time": "2024-08-03T14:20:22.047572Z"
    }
   },
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to read and clean text files\n",
    "def read_text_files(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read().strip()\n",
    "                if content:  # Check if the file is not empty\n",
    "                    cleaned_content = clean_text(content)\n",
    "                    texts.append(cleaned_content)\n",
    "    return texts\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove unwanted characters\n",
    "    unwanted_chars = ['*', '#', '_', ')', '(', '!', '?', '.', ',', '-']\n",
    "    for char in unwanted_chars:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read Dataset and Train SentencePiece Tokenizer",
   "id": "8e8e26cf09a85db2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T14:20:23.613278Z",
     "start_time": "2024-08-03T14:20:23.505567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read and clean dataset\n",
    "folder_path = './Dataset/nlp_dataset'\n",
    "texts = read_text_files(folder_path)\n",
    "\n",
    "# Save the cleaned texts to a temporary file for SentencePiece training\n",
    "with open(\"cleaned_texts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in texts:\n",
    "        f.write(f\"{text}\\n\")"
   ],
   "id": "c0d3a36bd2d8b805",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T14:20:26.793386Z",
     "start_time": "2024-08-03T14:20:26.781989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_txt_files(folder_path):\n",
    "    return sum(1 for filename in os.listdir(folder_path) if filename.endswith(\".txt\"))\n",
    "\n",
    "num_files = count_txt_files(folder_path)\n",
    "print(f\"Number of .txt files in folder: {num_files}\")"
   ],
   "id": "101c650bf6ef8d92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of .txt files in folder: 1000\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T14:20:27.291605Z",
     "start_time": "2024-08-03T14:20:27.192244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "texts = read_text_files(folder_path)\n",
    "print(f\"Number of texts read: {len(texts)}\")"
   ],
   "id": "1218cc8c8e2dc017",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts read: 579\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T14:20:31.951130Z",
     "start_time": "2024-08-03T14:20:29.232215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tentukan ukuran kosakata yang sesuai dengan jumlah token unik dalam data\n",
    "# Misalnya, kita gunakan 6000 sebagai ukuran kosakata yang lebih kecil dari 6111\n",
    "vocab_size = 6000\n",
    "\n",
    "# Train SentencePiece model\n",
    "spm.SentencePieceTrainer.train(input='cleaned_texts.txt', model_prefix='m', vocab_size=vocab_size)\n",
    "\n",
    "# Load the SentencePiece model\n",
    "sp = spm.SentencePieceProcessor(model_file='m.model')\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_texts = [sp.encode(text, out_type=int) for text in texts]\n",
    "\n",
    "# Prepare the data for TensorFlow\n",
    "tokenized_texts = [np.array(text) for text in tokenized_texts]\n",
    "\n",
    "# Convert tokenized texts to tensor format\n",
    "vectorized_texts = tf.ragged.constant(tokenized_texts, dtype=tf.int64).to_tensor()\n",
    "\n",
    "# Prepare the dataset for training\n",
    "batch_size = 8  # Mengurangi ukuran batch lebih lanjut\n",
    "dataset = tf.data.Dataset.from_tensor_slices((vectorized_texts, vectorized_texts))\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)"
   ],
   "id": "4e801a40a309bc1a",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define Positional Encoding and Transformer Block",
   "id": "6f5f10a602fcb9a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T14:20:59.143879Z",
     "start_time": "2024-08-03T14:20:59.128280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Positional Encoding\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "    \n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
    "                                     np.arange(d_model)[np.newaxis, :],\n",
    "                                     d_model)\n",
    "        \n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "\n",
    "# Define Transformer Block\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output = self.mha(x, x, x, attention_mask=mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        return out2\n"
   ],
   "id": "cc38c6cb63e086f0",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define Transformer Model",
   "id": "ca6f96dd16cb1e50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T14:41:56.440887Z",
     "start_time": "2024-08-03T14:41:56.416104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the full Transformer model\n",
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.encoder_embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(pe_input, d_model)\n",
    "        \n",
    "        self.enc_layers = [TransformerBlock(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dec_layers = [TransformerBlock(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def create_padding_mask(self, seq):\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        inp, tar = inputs\n",
    "        \n",
    "        enc_padding_mask = self.create_padding_mask(inp)\n",
    "        look_ahead_mask = self.create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_padding_mask = self.create_padding_mask(tar)\n",
    "\n",
    "        inp = self.encoder_embedding(inp)  # (batch_size, input_seq_len, d_model)\n",
    "        tar = self.decoder_embedding(tar)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        inp = self.pos_encoding(inp)\n",
    "        tar = self.pos_encoding(tar)\n",
    "\n",
    "        for enc_layer in self.enc_layers:\n",
    "            inp = enc_layer(inp, training, enc_padding_mask)\n",
    "\n",
    "        for dec_layer in self.dec_layers:\n",
    "            tar = dec_layer(tar, training, look_ahead_mask)\n",
    "\n",
    "        final_output = self.final_layer(tar)  # (batch_size, target_seq_len, target_vocab_size)\n",
    "        \n",
    "        return final_output\n"
   ],
   "id": "18ab634d120151ab",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Compile and Train the Model",
   "id": "f3d5cc13a7fa123"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T14:41:57.171638Z",
     "start_time": "2024-08-03T14:41:56.752860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "num_layers = 4  # Asli: 12, disesuaikan menjadi 8, kemudian 6, dan akhirnya 4\n",
    "d_model = 128   # Asli: 768, disesuaikan menjadi 256, kemudian 128\n",
    "num_heads = 4   # Asli: 12, disesuaikan menjadi 8, kemudian 4\n",
    "dff = 512       # Asli: 3072, disesuaikan menjadi 2048, kemudian 1024, dan akhirnya 512\n",
    "input_vocab_size = 6000  # Tetap\n",
    "target_vocab_size = 6000  # Tetap\n",
    "pe_input = 5000  # Kurangi nilai pe_input\n",
    "pe_target = 5000  # Kurangi nilai pe_target\n",
    "\n",
    "# Instantiate and compile the model\n",
    "transformer = TransformerModel(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target)\n",
    "\n",
    "# Define the learning rate schedule\n",
    "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "transformer.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Mengurangi ukuran batch menjadi 8\n",
    "batch_size = 8\n",
    "\n",
    "# Build the model by calling it on a batch of data\n",
    "input_batch = tf.random.uniform((batch_size, 256), dtype=tf.int64, minval=0, maxval=input_vocab_size)\n",
    "target_batch = tf.random.uniform((batch_size, 256), dtype=tf.int64, minval=0, maxval=target_vocab_size)\n",
    "\n",
    "_ = transformer((input_batch, target_batch), training=False)\n",
    "\n",
    "# Display model summary\n",
    "transformer.summary()\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "history = transformer.fit(dataset, epochs=epochs)\n",
    "\n",
    "# Save the model\n",
    "model_save_path = './models/complex_transformer_nlp_model.h5'\n",
    "transformer.save(model_save_path)\n"
   ],
   "id": "116ad58e764380fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_model_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_40 (Embedding)    multiple                  768000    \n",
      "                                                                 \n",
      " embedding_41 (Embedding)    multiple                  768000    \n",
      "                                                                 \n",
      " positional_encoding_20 (Pos  multiple                 0         \n",
      " itionalEncoding)                                                \n",
      "                                                                 \n",
      " transformer_block_264 (Tran  multiple                 396032    \n",
      " sformerBlock)                                                   \n",
      "                                                                 \n",
      " transformer_block_265 (Tran  multiple                 396032    \n",
      " sformerBlock)                                                   \n",
      "                                                                 \n",
      " transformer_block_266 (Tran  multiple                 396032    \n",
      " sformerBlock)                                                   \n",
      "                                                                 \n",
      " transformer_block_267 (Tran  multiple                 396032    \n",
      " sformerBlock)                                                   \n",
      "                                                                 \n",
      " transformer_block_268 (Tran  multiple                 396032    \n",
      " sformerBlock)                                                   \n",
      "                                                                 \n",
      " transformer_block_269 (Tran  multiple                 396032    \n",
      " sformerBlock)                                                   \n",
      "                                                                 \n",
      " transformer_block_270 (Tran  multiple                 396032    \n",
      " sformerBlock)                                                   \n",
      "                                                                 \n",
      " transformer_block_271 (Tran  multiple                 396032    \n",
      " sformerBlock)                                                   \n",
      "                                                                 \n",
      " dropout_643 (Dropout)       multiple                  0 (unused)\n",
      "                                                                 \n",
      " dense_563 (Dense)           multiple                  774000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,478,256\n",
      "Trainable params: 5,478,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer \"transformer_model_20\" (type TransformerModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_19748\\2130157609.py\", line 27, in call  *\n            inp, tar = inputs\n    \n        OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n    \n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(8, 4656), dtype=int64)\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOperatorNotAllowedInGraphError\u001B[0m            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[62], line 40\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m     39\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[1;32m---> 40\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mtransformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;66;03m# Save the model\u001B[39;00m\n\u001B[0;32m     43\u001B[0m model_save_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./models/complex_transformer_nlp_model.h5\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[0;32m   1146\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1147\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mag_error_metadata\u001B[38;5;241m.\u001B[39mto_exception(e)\n\u001B[0;32m   1148\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1149\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mOperatorNotAllowedInGraphError\u001B[0m: in user code:\n\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer \"transformer_model_20\" (type TransformerModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_19748\\2130157609.py\", line 27, in call  *\n            inp, tar = inputs\n    \n        OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n    \n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(8, 4656), dtype=int64)\n      • training=True\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T14:17:35.784726Z",
     "start_time": "2024-08-03T14:17:35.615612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "epochs = 10\n",
    "history = transformer.fit(dataset, epochs=epochs)\n",
    "\n",
    "# Save the model\n",
    "model_save_path = './models/complex_transformer_nlp_model.h5'\n",
    "transformer.save(model_save_path)"
   ],
   "id": "ee8288389e12cb2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer \"transformer_model_13\" (type TransformerModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_19748\\2130157609.py\", line 27, in call  *\n            inp, tar = inputs\n    \n        OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n    \n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(8, 4656), dtype=int64)\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOperatorNotAllowedInGraphError\u001B[0m            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[42], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m      2\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[1;32m----> 3\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mtransformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Save the model\u001B[39;00m\n\u001B[0;32m      6\u001B[0m model_save_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./models/complex_transformer_nlp_model.h5\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[0;32m   1146\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1147\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mag_error_metadata\u001B[38;5;241m.\u001B[39mto_exception(e)\n\u001B[0;32m   1148\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1149\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mOperatorNotAllowedInGraphError\u001B[0m: in user code:\n\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer \"transformer_model_13\" (type TransformerModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_19748\\2130157609.py\", line 27, in call  *\n            inp, tar = inputs\n    \n        OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n    \n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(8, 4656), dtype=int64)\n      • training=True\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T14:08:46.253777Z",
     "start_time": "2024-08-03T14:08:46.222525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Memastikan TensorFlow mendeteksi GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ],
   "id": "9f872c53bb7dc5ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "99a73a9685ae1499"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
