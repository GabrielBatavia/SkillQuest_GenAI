{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T08:19:24.320365Z",
     "start_time": "2024-07-18T08:19:24.310325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ],
   "id": "ff6e7760bc7b7f3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T10:36:07.958752Z",
     "start_time": "2024-07-18T10:36:07.934562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ],
   "id": "f9c336c23c98501f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Persiapan Dataset",
   "id": "fcf86e1dc248d552"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T10:30:46.193719Z",
     "start_time": "2024-07-18T10:30:46.113542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def load_dataset(folder_path):\n",
    "    dataset = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, dict):  # Check if the data is a dictionary\n",
    "                        dataset.append(data)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping file {file_path}, not a valid JSON.\")\n",
    "    return dataset\n",
    "folder_path = './Dataset/generative_dataset'\n",
    "dataset = load_dataset(folder_path)"
   ],
   "id": "39ea9bf9f17ae3d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file ./Dataset/generative_dataset\\0312.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0540.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0541.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0542.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0543.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0544.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0545.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0546.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0547.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0548.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0549.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0550.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0551.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0552.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0553.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0554.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0555.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0556.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0557.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0558.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0559.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0560.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0561.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0562.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0563.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0564.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0565.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0566.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0567.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0568.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0569.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0570.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0571.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0572.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0573.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0574.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0575.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0576.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0577.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0578.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0579.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0580.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0581.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0582.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0583.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0584.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0585.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0586.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0587.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0588.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0589.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0590.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0591.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0592.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0593.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0594.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0595.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0596.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0597.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0598.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0599.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0600.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0601.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0602.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0603.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0604.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0605.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0606.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0607.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0608.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0609.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0610.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0611.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0612.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0613.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0614.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0615.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0616.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0617.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0618.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0619.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0620.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0621.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0622.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0623.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0624.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0625.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0626.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0627.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0628.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0629.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0630.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0631.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0632.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0633.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0634.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0635.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0636.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0637.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0638.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0639.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0640.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0641.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0642.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0643.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0644.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0645.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0646.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0647.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0648.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0649.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0650.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0651.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0652.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0653.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0654.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0655.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0656.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0657.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0658.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0659.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0660.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0661.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0662.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0663.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0664.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0665.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0666.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0667.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0668.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0669.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0670.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0671.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0672.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0673.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0674.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0675.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0676.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0677.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0678.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0679.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0680.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0681.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0682.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0683.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0684.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0685.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0686.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0687.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0688.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0689.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0690.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0691.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0692.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0693.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0694.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0695.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0696.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0697.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0698.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0699.json, not a valid JSON.\n",
      "Skipping file ./Dataset/generative_dataset\\0700.json, not a valid JSON.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenisasi",
   "id": "2d6f4f07eae7aa8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T10:34:51.386468Z",
     "start_time": "2024-07-18T10:34:51.330499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.word2index = defaultdict(int)\n",
    "        self.index2word = []\n",
    "        self.word_count = 0\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        for text in texts:\n",
    "            for word in re.findall(r'\\w+', text):\n",
    "                if word not in self.word2index:\n",
    "                    self.word2index[word] = self.word_count\n",
    "                    self.index2word.append(word)\n",
    "                    self.word_count += 1\n",
    "    \n",
    "    def transform(self, text):\n",
    "        return [self.word2index[word] for word in re.findall(r'\\w+', text)]\n",
    "    \n",
    "    def fit_transform(self, texts):\n",
    "        self.fit(texts)\n",
    "        return [self.transform(text) for text in texts]\n",
    "    \n",
    "    def save(self, path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump({'word2index': self.word2index, 'index2word': self.index2word}, f)\n",
    "    \n",
    "    def load(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            self.word2index = data['word2index']\n",
    "            self.index2word = data['index2word']\n",
    "            self.word_count = len(self.index2word)\n",
    "\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "# Mengumpulkan semua teks untuk tokenisasi\n",
    "texts = []\n",
    "for entry in dataset:\n",
    "    if 'judul' in entry and 'konten' in entry and 'contoh' in entry:\n",
    "        texts.append(entry['judul'])\n",
    "        texts.append(entry['konten'])\n",
    "        for example in entry['contoh']:\n",
    "            if 'isi' in example:\n",
    "                texts.append(example['isi'])\n",
    "\n",
    "tokenizer.fit(texts)\n",
    "\n",
    "# Membuat tokenized_data dengan struktur yang benar\n",
    "tokenized_data = []\n",
    "for entry in dataset:\n",
    "    if 'judul' in entry and 'konten' in entry and 'contoh' in entry:\n",
    "        title_tokens = tokenizer.transform(entry['judul'])\n",
    "        content_tokens = tokenizer.transform(entry['konten'])\n",
    "        tokenized_data.append({\n",
    "            'title_tokens': {'input_ids': title_tokens},\n",
    "            'content_tokens': {'input_ids': content_tokens}\n",
    "        })\n",
    "\n",
    "\n",
    "# Simpan tokenizer\n",
    "tokenizer.save('./saved_model/token/simple_tokenizer.json')"
   ],
   "id": "f38b33bd36489601",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model",
   "id": "96965c0ffd6c5ab4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T10:34:52.977871Z",
     "start_time": "2024-07-18T10:34:52.720231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, max_seq_length=512):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, d_model))\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, batch_first=True)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        tgt = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
    "        output = self.transformer(src, tgt)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "vocab_size = len(tokenizer.index2word)\n",
    "model = TransformerModel(vocab_size)\n"
   ],
   "id": "7b814d0bdb3fac04",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train Model",
   "id": "2573abc41910ceba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T10:35:29.777896Z",
     "start_time": "2024-07-18T10:34:54.296693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src = [item['src'] for item in batch]\n",
    "    tgt = [item['tgt'] for item in batch]\n",
    "    src = torch.nn.utils.rnn.pad_sequence(src, padding_value=0, batch_first=True)\n",
    "    tgt = torch.nn.utils.rnn.pad_sequence(tgt, padding_value=0, batch_first=True)\n",
    "    return src, tgt\n",
    "\n",
    "train_data = [{'src': torch.tensor(entry['title_tokens']['input_ids']), 'tgt': torch.tensor(entry['content_tokens']['input_ids'])} for entry in tokenized_data]\n",
    "train_dataset = CustomDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(30):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        loss = criterion(output.view(-1, vocab_size), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), './saved_model/transformer_model.pth')\n"
   ],
   "id": "4f459d249d58b5c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.874313286372593\n",
      "Epoch 2, Loss: 6.200914178575788\n",
      "Epoch 3, Loss: 5.853746959141323\n",
      "Epoch 4, Loss: 5.4872443335396905\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 37\u001B[0m\n\u001B[0;32m     35\u001B[0m output \u001B[38;5;241m=\u001B[39m model(src, tgt[:, :\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m     36\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, vocab_size), tgt[:, \u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m---> 37\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     39\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    524\u001B[0m     )\n\u001B[1;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    745\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    746\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "21b8a16890fb7195"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
