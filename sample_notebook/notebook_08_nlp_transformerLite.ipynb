{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Arsitektur Model",
   "id": "96ea34c773cd2b1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    +--------------------------------------------------------------------------------------------------+\n",
    "    |                                            Transformer Lite Model                                |\n",
    "    +--------------------------------------------------------------------------------------------------+\n",
    "    |                                                                                                  |\n",
    "    | +---------------------+   +---------------------+    +---------------------+    +----------------+|\n",
    "    | |  Input Embedding    |   |  Positional Encoding |    |  Encoder Layer 1    |    |  Decoder Layer 1|\n",
    "    | |                     |   |                     |    |                     |    |                ||\n",
    "    | | +----------------+  |   | +-----------------+ |    | +----------------+  |    | +-------------+ ||\n",
    "    | | | Token Embedding |  |   | | Positional     | |    | | Self-Attention |  |    | | Masked      | ||\n",
    "    | | |                |  |   | | Encoding       | |    | |                |  |    | | Self-        | ||\n",
    "    | | |                |  |   | |                | |    | |                |  |    | | Attention   | ||\n",
    "    | | +----------------+  |   | +-----------------+ |    | +----------------+  |    | +-------------+ ||\n",
    "    | +---------------------+   +---------------------+    +---------------------+    +----------------+|\n",
    "    |                                                                                                  |\n",
    "    | +---------------------+   +---------------------+    +---------------------+    +----------------+|\n",
    "    | |   Encoder Layer 2   |   |  Decoder Layer 2    |    |  Encoder Layer N    |    |  Decoder Layer N|\n",
    "    | |                     |   |                     |    |                     |    |                ||\n",
    "    | | +----------------+  |   | +-----------------+ |    | +----------------+  |    | +-------------+ ||\n",
    "    | | | Self-Attention |  |   | | Masked          | |    | | Self-Attention |  |    | | Masked      | ||\n",
    "    | | |                |  |   | | Self-Attention  | |    | |                |  |    | | Self-        | ||\n",
    "    | | |                |  |   | |                | |    | |                |  |    | | Attention   | ||\n",
    "    | | +----------------+  |   | +-----------------+ |    | +----------------+  |    | +-------------+ ||\n",
    "    | +---------------------+   +---------------------+    +---------------------+    +----------------+|\n",
    "    |                                                                                                  |\n",
    "    | +---------------------+   +---------------------+    +---------------------+                      |\n",
    "    | | Final Linear Layer  |   |   Output           |    |   Prediction        |                      |\n",
    "    | |                     |   |                    |    |                     |                      |\n",
    "    | | +----------------+  |   | +-----------------+ |    | +----------------+  |                      |\n",
    "    | | | Linear Layer   |  |   | | Softmax         | |    | | Final Output   |  |                      |\n",
    "    | | |                |  |   | |                | |    | |                |  |                      |\n",
    "    | | +----------------+  |   | +-----------------+ |    | +----------------+  |                      |\n",
    "    | +---------------------+   +---------------------+    +---------------------+                      |\n",
    "    +--------------------------------------------------------------------------------------------------+\n"
   ],
   "id": "ba144dc35aa48ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Penjelasan\n",
    "\n",
    "### Input Embedding\n",
    "Token Embedding: Mengubah token dari teks input menjadi vektor numerik yang dapat diproses oleh jaringan saraf.\n",
    "\n",
    "Positional Encoding: Menambahkan informasi posisi ke setiap token embedding untuk menjaga urutan token dalam urutan input.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "Self-Attention Mechanism: Mempelajari representasi token dengan mempertimbangkan semua token dalam urutan input untuk menangkap konteks global.\n",
    "\n",
    "Feed-Forward Network: Lapisan feed-forward jaringan saraf untuk memproses representasi token lebih lanjut.\n",
    "\n",
    "Layer Normalization dan Dropout: Membantu stabilisasi dan generalisasi model.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "Masked Self-Attention Mechanism: Serupa dengan self-attention pada encoder, tetapi dengan masking untuk memastikan prediksi token hanya bergantung pada token sebelumnya dalam urutan.\n",
    "\n",
    "Self-Attention Mechanism: Mengintegrasikan informasi dari urutan input dan urutan output yang dihasilkan sejauh ini.\n",
    "\n",
    "Feed-Forward Network: Seperti pada encoder, digunakan untuk memproses representasi token lebih lanjut.\n",
    "\n",
    "Layer Normalization dan Dropout: Sama seperti pada encoder, digunakan untuk stabilisasi dan generalisasi.\n",
    "\n",
    "### Output\n",
    "\n",
    "Final Linear Layer: Mengubah representasi token dari decoder menjadi distribusi probabilitas token output.\n",
    "\n",
    "Softmax Layer: Menghasilkan prediksi probabilitas token output.\n",
    "\n",
    "Prediction: Token dengan probabilitas tertinggi dipilih sebagai prediksi akhir.\n",
    "\n",
    "## Kelebihan Transformer Lite\n",
    "\n",
    "Efisiensi: Menggunakan arsitektur ringan yang lebih efisien dalam penggunaan memori dan komputasi, cocok untuk perangkat dengan sumber daya terbatas seperti GPU dengan memori terbatas.\n",
    "\n",
    "Kemampuan Pemahaman Konteks: Menggunakan mekanisme perhatian yang kuat untuk menangkap hubungan konteks antar-token dalam urutan input.\n",
    "\n",
    "Fleksibilitas: Dapat disesuaikan dengan berbagai tugas NLP seperti penerjemahan, pemrosesan teks, dan lain-lain."
   ],
   "id": "beddffd83ab0367a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries and Define Helper Functions",
   "id": "a98da58d081a1626"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-04T05:43:47.666657Z",
     "start_time": "2024-08-04T05:43:42.899923Z"
    }
   },
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to read and clean text files\n",
    "def read_text_files(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read().strip()\n",
    "                if content:  # Check if the file is not empty\n",
    "                    cleaned_content = clean_text(content)\n",
    "                    texts.append(cleaned_content)\n",
    "    return texts\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove unwanted characters\n",
    "    unwanted_chars = ['*', '#', '_', ')', '(', '!', '?', '.', ',', '-']\n",
    "    for char in unwanted_chars:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Read Dataset and Train SentencePiece Tokenizer",
   "id": "3518e83e92df867f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T05:43:47.928937Z",
     "start_time": "2024-08-04T05:43:47.667726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read and clean dataset\n",
    "folder_path = './Dataset/nlp_dataset'\n",
    "texts = read_text_files(folder_path)\n",
    "\n",
    "# Save the cleaned texts to a temporary file for SentencePiece training\n",
    "with open(\"cleaned_texts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in texts:\n",
    "        f.write(f\"{text}\\n\")"
   ],
   "id": "1a385f531e724458",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T05:50:02.333398Z",
     "start_time": "2024-08-04T05:50:00.125269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tentukan ukuran kosakata yang sesuai dengan jumlah token unik dalam data\n",
    "vocab_size = 6205\n",
    "\n",
    "# Train SentencePiece model\n",
    "spm.SentencePieceTrainer.train(input='cleaned_texts.txt', model_prefix='m', vocab_size=vocab_size)\n",
    "\n",
    "# Load the SentencePiece model\n",
    "sp = spm.SentencePieceProcessor(model_file='m.model')\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_texts = [sp.encode(text, out_type=int) for text in texts]\n",
    "\n",
    "# Prepare the data for TensorFlow\n",
    "tokenized_texts = [np.array(text) for text in tokenized_texts]\n",
    "\n",
    "# Menghitung panjang maksimum sequence dalam dataset\n",
    "max_seq_len = max(len(seq) for seq in tokenized_texts)\n",
    "\n",
    "# Padding semua sequence ke panjang maksimum\n",
    "padded_texts = [np.pad(seq, (0, max_seq_len - len(seq)), 'constant') for seq in tokenized_texts]\n",
    "\n",
    "# Konversi sequence ke tensor\n",
    "vectorized_texts = tf.convert_to_tensor(padded_texts, dtype=tf.int64)\n",
    "\n",
    "# Siapkan dataset untuk pelatihan\n",
    "batch_size = 8  # Mengurangi ukuran batch lebih lanjut\n",
    "dataset = tf.data.Dataset.from_tensor_slices((vectorized_texts, vectorized_texts))\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)"
   ],
   "id": "12637be42e63058c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Positional Encoding and Transformer Lite Block",
   "id": "b6ca37cba474d1f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T05:50:03.817231Z",
     "start_time": "2024-08-04T05:50:03.790177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Positional Encoding\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "    \n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
    "                                     np.arange(d_model)[np.newaxis, :],\n",
    "                                     d_model)\n",
    "        \n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "\n",
    "# Define Transformer Lite Block\n",
    "class TransformerLiteBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerLiteBlock, self).__init__()\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output = self.mha(x, x, x, attention_mask=mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        return out2\n"
   ],
   "id": "aa4836982fd70a72",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Transformer Lite Model",
   "id": "bf0d2257a825202"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T05:51:01.664780Z",
     "start_time": "2024-08-04T05:51:01.654604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerLiteModel(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(TransformerLiteModel, self).__init__()\n",
    "\n",
    "        self.encoder_embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(pe_input, d_model)\n",
    "        \n",
    "        self.enc_layers = [TransformerLiteBlock(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dec_layers = [TransformerLiteBlock(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def create_padding_mask(self, seq):\n",
    "        # Pastikan seq memiliki dimensi yang benar\n",
    "        if len(seq.shape) == 1:\n",
    "            seq = tf.expand_dims(seq, axis=0)\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        inp = inputs[0]\n",
    "        tar = inputs[1]\n",
    "        \n",
    "        # Pastikan inp dan tar memiliki setidaknya 2 dimensi\n",
    "        if len(inp.shape) == 1:\n",
    "            inp = tf.expand_dims(inp, axis=0)\n",
    "        if len(tar.shape) == 1:\n",
    "            tar = tf.expand_dims(tar, axis=0)\n",
    "\n",
    "        enc_padding_mask = self.create_padding_mask(inp)\n",
    "        look_ahead_mask = self.create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_padding_mask = self.create_padding_mask(tar)\n",
    "\n",
    "        inp = self.encoder_embedding(inp)  # (batch_size, input_seq_len, d_model)\n",
    "        tar = self.decoder_embedding(tar)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        inp = self.pos_encoding(inp)\n",
    "        tar = self.pos_encoding(tar)\n",
    "\n",
    "        for enc_layer in self.enc_layers:\n",
    "            inp = enc_layer(inp, training, enc_padding_mask)\n",
    "\n",
    "        for dec_layer in self.dec_layers:\n",
    "            tar = dec_layer(tar, training, look_ahead_mask)\n",
    "\n",
    "        final_output = self.final_layer(tar)  # (batch_size, target_seq_len, target_vocab_size)\n",
    "        \n",
    "        return final_output"
   ],
   "id": "5213165f9520bc82",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compile and Train the Model",
   "id": "5c83a4e4a1e312ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T05:51:02.938331Z",
     "start_time": "2024-08-04T05:51:02.581649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "num_layers = 4  # Asli: 12, disesuaikan menjadi 4\n",
    "d_model = 128   # Asli: 768, disesuaikan menjadi 128\n",
    "num_heads = 4   # Asli: 12, disesuaikan menjadi 4\n",
    "dff = 512       # Asli: 3072, disesuaikan menjadi 512\n",
    "input_vocab_size = 6000  # Tetap\n",
    "target_vocab_size = 6000  # Tetap\n",
    "pe_input = 5000  # Kurangi nilai pe_input\n",
    "pe_target = 5000  # Kurangi nilai pe_target\n",
    "\n",
    "# Instantiate and compile the model\n",
    "transformer_lite = TransformerLiteModel(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target)\n",
    "\n",
    "# Define the learning rate schedule\n",
    "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "transformer_lite.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Mengurangi ukuran batch menjadi 8\n",
    "batch_size = 8\n",
    "\n",
    "# Build the model by calling it on a batch of data\n",
    "input_batch = tf.random.uniform((batch_size, 256), dtype=tf.int64, minval=0, maxval=input_vocab_size)\n",
    "target_batch = tf.random.uniform((batch_size, 256), dtype=tf.int64, minval=0, maxval=target_vocab_size)\n",
    "\n",
    "_ = transformer_lite((input_batch, target_batch), training=False)\n",
    "\n",
    "# Display model summary\n",
    "transformer_lite.summary()"
   ],
   "id": "ea0de8bc78e59c88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_lite_model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     multiple                  768000    \n",
      "                                                                 \n",
      " embedding_7 (Embedding)     multiple                  768000    \n",
      "                                                                 \n",
      " positional_encoding_3 (Posi  multiple                 0         \n",
      " tionalEncoding)                                                 \n",
      "                                                                 \n",
      " transformer_lite_block_24 (  multiple                 396032    \n",
      " TransformerLiteBlock)                                           \n",
      "                                                                 \n",
      " transformer_lite_block_25 (  multiple                 396032    \n",
      " TransformerLiteBlock)                                           \n",
      "                                                                 \n",
      " transformer_lite_block_26 (  multiple                 396032    \n",
      " TransformerLiteBlock)                                           \n",
      "                                                                 \n",
      " transformer_lite_block_27 (  multiple                 396032    \n",
      " TransformerLiteBlock)                                           \n",
      "                                                                 \n",
      " transformer_lite_block_28 (  multiple                 396032    \n",
      " TransformerLiteBlock)                                           \n",
      "                                                                 \n",
      " transformer_lite_block_29 (  multiple                 396032    \n",
      " TransformerLiteBlock)                                           \n",
      "                                                                 \n",
      " transformer_lite_block_30 (  multiple                 396032    \n",
      " TransformerLiteBlock)                                           \n",
      "                                                                 \n",
      " transformer_lite_block_31 (  multiple                 396032    \n",
      " TransformerLiteBlock)                                           \n",
      "                                                                 \n",
      " dropout_91 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      " dense_67 (Dense)            multiple                  774000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,478,256\n",
      "Trainable params: 5,478,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T05:51:04.825193Z",
     "start_time": "2024-08-04T05:51:03.711792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "epochs = 10\n",
    "history = transformer_lite.fit(dataset, epochs=epochs)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "model_save_path = './models/transformer_lite_nlp_model.h5'\n",
    "transformer_lite.save(model_save_path)"
   ],
   "id": "ae1699e42478bfe7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\losses.py\", line 1862, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend.py\", line 5202, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(8, 4653) and logits.shape=(1, 4653, 6000)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m      2\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[1;32m----> 3\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mtransformer_lite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Save the model\u001B[39;00m\n\u001B[0;32m      7\u001B[0m model_save_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./models/transformer_lite_nlp_model.h5\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[0;32m   1146\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1147\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mag_error_metadata\u001B[38;5;241m.\u001B[39mto_exception(e)\n\u001B[0;32m   1148\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1149\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\losses.py\", line 1862, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\gabri\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend.py\", line 5202, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(8, 4653) and logits.shape=(1, 4653, 6000)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9ed8d9a4c800fdd2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
